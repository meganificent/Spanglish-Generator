{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project",
      "provenance": [],
      "collapsed_sections": [
        "f4yH_HBz0BZf",
        "t-kpFQz7J6uA",
        "K0AR8EKfCHbP"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meganificent/Language-Style-Transfer/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTGBelac_NaI"
      },
      "source": [
        "1.\tInsert English phrase\n",
        "a.\tTokenize English phrase --> get POS array of tuple\n",
        "2.\tTranslate English phrase to Spanish using Google Translate\n",
        "3.\tToken Spanish phrase\n",
        "4.\tSpaghetti tokenized Spanish phrase --> output array of tuples with Spanish word as the key and pos as the value\n",
        "5.\tGo through each key of POS English phrase and compare to keys of POS Spanish phrases --> move all English object intervals so that they are in the same positions as their translated Spanish word counterparts\n",
        "6.\tMerge rearranged array of objects so that their keys produce one potentially grammatically incoherent English sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl8B52Ad_WED"
      },
      "source": [
        "Special cases: \"a\" in spanish, \"to\" for location in English --> brainstorm placeholders for these kinds of things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12swv4FovvTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2acb1fd3-35a0-40d9-de7d-a3e47dc35c7c"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "import numpy"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNrrF_WYDXEP"
      },
      "source": [
        "from __future__ import print_function"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOSsb-duk_Ot",
        "outputId": "c06813af-6ca2-4524-e36c-466e759f9962"
      },
      "source": [
        "import spaghetti as sp\n",
        "import nltk\n",
        "nltk.download('cess_esp')\n",
        "mytagger = sp.CESSTagger()\n",
        "mytagger_uni = mytagger.uni\n",
        "mytagger_bi = mytagger.bi"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Package cess_esp is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G944WiS3dM82"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ippBa9j5chha",
        "outputId": "365bd7b1-bc1c-427d-a198-5c188d43d4be"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "syns = wordnet.synsets(\"up\")\n",
        "print(syns[2].lemmas()[0].antonyms())"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBDrN7PddoFJ",
        "outputId": "1cccece8-3375-4c6e-e63a-8995204f29af"
      },
      "source": [
        "\tsynonyms = []\n",
        "\tantonyms = []\n",
        "\n",
        "\tfor syn in wordnet.synsets(\"none\"):\n",
        "\t\tfor l in syn.lemmas():\n",
        "\t\t\tsynonyms.append(l.name())\n",
        "\t\t\tif l.antonyms():\n",
        "\t\t\t\t antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "\tprint(set(synonyms))\n",
        "\tprint(set(antonyms))"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'none'}\n",
            "set()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wXdg027CAaA"
      },
      "source": [
        "### **Google Translate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gupDVPEVCNXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272848e5-0343-45d2-a874-5dfc945c792c"
      },
      "source": [
        "!pip install googletrans==3.1.0a0\n",
        "import googletrans\n",
        "print(googletrans.LANGUAGES)\n",
        "from googletrans import Translator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.22)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.4.0)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "{'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'he': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'or': 'odia', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'ug': 'uyghur', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMZv6MloFg0y"
      },
      "source": [
        "translator = Translator(service_urls=['translate.googleapis.com'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms4UFFebztCW",
        "outputId": "5711866a-5028-4cc0-9575-eaba716ad1d9"
      },
      "source": [
        "result = translator.translate(\"el perro blanco corrió\", dest=\"en\")\n",
        "print(result.text)\n",
        "print(result.src)\n",
        "print(result.dest)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the white dog ran\n",
            "es\n",
            "en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nZJ6VMCwta7",
        "outputId": "071a9ee0-8160-4b2c-a0e6-e7babbdbe6df"
      },
      "source": [
        "spanglishMachine(\"white coffee cup spilled it\")"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: [('white', 'JJ'), ('coffee', 'NN'), ('cup', 'NN'), ('spilled', 'VBD'), ('it', 'PRP')]\n",
            "Remove Pronouns Before Verbs: [('white', 'JJ'), ('coffee', 'NN'), ('cup', 'NN'), ('spilled', 'VBD'), ('it', 'PRP')]\n",
            "Pronoun+Verb+Noun: [('it', 'PRP'), ('spilled', 'VBD'), ('white', 'JJ'), ('coffee', 'NN'), ('cup', 'NN')]\n",
            "Adjectives after Nouns: [('it', 'PRP'), ('spilled', 'VBD'), ('coffee', 'NN'), ('cup', 'NN'), ('white', 'JJ')]\n",
            "Add 'the' [('it', 'PRP'), ('spilled', 'VBD'), ('the', 'DT'), ('coffee', 'NN'), ('cup', 'NN'), ('white', 'JJ')]\n",
            "Attributive Nouns: [('it', 'PRP'), ('spilled', 'VBD'), ('the', 'DT'), ('cup', 'NN'), ('of', 'IN'), ('coffee', 'NN'), ('white', 'JJ')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnV9pAZlq1Mc",
        "outputId": "1bfe385c-180f-4c02-adbf-d1b75fd30822"
      },
      "source": [
        "spanglishMachine(\"Megan wrote it\")"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: [('Megan', 'NNP'), ('wrote', 'VBD'), ('it', 'PRP')]\n",
            "Remove Pronouns Before Verbs: [('Megan', 'NNP'), ('wrote', 'VBD'), ('it', 'PRP')]\n",
            "Pronoun+Verb+Noun: [('it', 'PRP'), ('wrote', 'VBD'), ('Megan', 'NNP')]\n",
            "Adjectives after Nouns: [('it', 'PRP'), ('wrote', 'VBD'), ('Megan', 'NNP')]\n",
            "Add 'the' [('it', 'PRP'), ('wrote', 'VBD'), ('Megan', 'NNP')]\n",
            "Attributive Nouns: [('it', 'PRP'), ('wrote', 'VBD'), ('Megan', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxtwzGXLwz9K",
        "outputId": "85d41ccb-5a1b-47c5-c0a2-3e809a4f2a1f"
      },
      "source": [
        "spanglishMachine(\"I write but the hot sun stays down\")"
      ],
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: [('I', 'PRP'), ('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('hot', 'JJ'), ('sun', 'NN'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "Remove Pronouns Before Verbs: [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('hot', 'JJ'), ('sun', 'NN'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "Pronoun+Verb+Noun: [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('hot', 'JJ'), ('sun', 'NN'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "Adjectives after Nouns: [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('sun', 'NN'), ('hot', 'JJ'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "Add 'the' [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('sun', 'NN'), ('hot', 'JJ'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "Attributive Nouns: [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('sun', 'NN'), ('hot', 'JJ'), ('stays', 'VBZ'), ('down', 'RP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60JXLI0rRHRD"
      },
      "source": [
        "def spanglishMachine(string): \n",
        "  origTokenized = nltk.word_tokenize(string)\n",
        "\n",
        "  #determine language of inserted string and feed into correct translation function\n",
        "  \n",
        "  translated = translator.translate(string, dest = \"en\") #initially translate to english\n",
        "  if translated.src == \"en\": #if the original language was english\n",
        "    tagged = nltk.pos_tag(origTokenized) #tag POS using nltk\n",
        "    print(\"Original:\", tagged)\n",
        "    toSpanishStyle(tagged) #transfer to Spanish\n",
        "\n",
        "  else: #if original languagse was spanish\n",
        "    tagged = sp.pos_tag(origTokenized) #tag POS using spaghetti\n",
        "    print(\"Original:\", tagged)\n",
        "    toEnglishStyle(tagged) #transfer to English\n",
        "\n"
      ],
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXAHHc3LoUsa",
        "outputId": "75d6a849-f5bf-4e5b-ae91-949d62d0e334"
      },
      "source": [
        "fi = [\"hi\", \"my\", \"tired\", \"mind\"]\n",
        "fi[3:3] = \"beautiful\", \"lost\"\n",
        "print(fi)\n",
        "fi[4:4] = fi[1:3]\n",
        "print(fi)\n",
        "del fi[1:3]\n",
        "print(fi)\n",
        "print(len(fi))"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hi', 'my', 'tired', 'beautiful', 'lost', 'mind']\n",
            "['hi', 'my', 'tired', 'beautiful', 'my', 'tired', 'lost', 'mind']\n",
            "['hi', 'beautiful', 'my', 'tired', 'lost', 'mind']\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqcAIiut2OT"
      },
      "source": [
        "def toSpanishStyle(taggedWords):\n",
        "  #Rule 1: Remove P\n",
        "  #taggedWords = [word for word in taggedWords if word[0] != \"I\"]\n",
        "\n",
        "  #Rule 1: Remove Pronouns Directly Before Conjugated Verbs\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1:\n",
        "    if taggedWords[idx][1] == \"PRP\":\n",
        "      if taggedWords[idx+1][1][:2] == \"VB\":\n",
        "        del taggedWords[idx]\n",
        "    idx += 1\n",
        "  print(\"Remove Pronouns Before Verbs:\", taggedWords)\n",
        "\n",
        "  #Rule 2: Double Negatives\n",
        "  #for word in taggedWords:\n",
        "    #if englishNegAndAff[0].index(word[0]):\n",
        "      #print(\"aah this scares me\")\n",
        "\n",
        "\n",
        "  #Rule 3: Pronoun + Verb + Noun\n",
        "  idx = 0\n",
        "  nounStart = 0\n",
        "  nounEnd = 0\n",
        "  while idx < len(taggedWords) - 1:\n",
        "    if taggedWords[idx][1][:2] == \"VB\": #if current word is a verb\n",
        "      #print(idx)\n",
        "      if taggedWords[idx+1][1] == \"PRP\": #if there's a pronoun directly after the verb\n",
        "        if idx-1 >= 0: #to avoid runtime errors\n",
        "          if taggedWords[idx-1][1][:2] == \"NN\": #if there's a noun right before the verb\n",
        "            nounStart = idx-1 #start of noun and its modifiers\n",
        "            nounEnd = idx # idx-1 + 1 for the sake of indexing later\n",
        "            if idx-2 >= 0: #to avoid runtime errors\n",
        "              if (taggedWords[idx-2][1][:2] == \"DT\") | (taggedWords[idx-2][1][:2] == \"JJ\") | (taggedWords[idx-2][1][:2] == \"NN\"): #if there's a determiner or adjective next to the noun\n",
        "                nounStart = idx-2\n",
        "                if idx-3 >= 0: #to avoid runtime errors\n",
        "                  if (taggedWords[idx-3][1][:2] == \"DT\") | (taggedWords[idx-3][1][:2] == \"JJ\"): #if there's a determiner next to the adjective\n",
        "                    nounStart = idx-3\n",
        "                  if idx-4 >=0:\n",
        "                    if taggedWords[idx-4][1][:2] == \"DT\":\n",
        "                      nounStart = idx-4\n",
        "          taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx] #swap pronoun and verb positions\n",
        "          taggedWords[idx+2:idx+2] = taggedWords[nounStart:nounEnd] #add noun stuff after where pronoun used to be\n",
        "          del taggedWords[nounStart:nounEnd] #remove ealier noun elements\n",
        "          idx +=1 #so I don't recount verb\n",
        "    idx +=1\n",
        "  print(\"Pronoun+Verb+Noun:\", taggedWords)\n",
        "\n",
        "  #Rule 4: Adjectives After Nouns\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1: #for every tuple (word) in Array\n",
        "    if taggedWords[idx][1][:2] == \"JJ\": #if current word is an adjective (can tell general tag by first 2 letters of acronym)\n",
        "      if taggedWords[idx+1][1][:2] == \"NN\": #if the proceding word is a noun #how do I avoid an error if adjective is last word in list?\n",
        "        taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx]\n",
        "    idx += 1\n",
        "  print(\"Adjectives after Nouns:\", taggedWords)\n",
        "\n",
        "  #Rule 5: \"The\" Before Every Non-Proper Noun\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords):\n",
        "    if (taggedWords[idx][1][:2] == \"NN\") & (taggedWords[idx][1][:3] != \"NNP\"): #if word is a noun and not a proper noun\n",
        "      if idx == 0:\n",
        "        taggedWords[idx:idx] = nltk.pos_tag(nltk.word_tokenize(\"the\"))\n",
        "        idx += 1 #so we don't recount nouns\n",
        "      else: #to avoid runtime errors\n",
        "        if (taggedWords[idx - 1][1] != \"DT\") & (taggedWords[idx - 1][1][:2] != \"NN\"): #if noun doesn't already have an \"a\" or \"the\" in front of it and if it doesn't have another noun in front of it, waiting to be turned into noun+of+noun\n",
        "          taggedWords[idx:idx] = nltk.pos_tag(nltk.word_tokenize(\"the\"))\n",
        "          idx += 1 #so we don't recount nouns\n",
        "    idx += 1\n",
        "  print(\"Add 'the'\", taggedWords)\n",
        "\n",
        "\n",
        "  #Rule 6: Attributive Nouns\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1:\n",
        "    if taggedWords[idx][1][:2] == \"NN\": #if current word is a noun\n",
        "      if taggedWords[idx+1][1][:2] == \"NN\": #if directly adjacent word is a noun\n",
        "        taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx] #swap the nouns\n",
        "        #print(taggedWords)\n",
        "        taggedWords[idx+1:idx+1] = nltk.pos_tag(nltk.word_tokenize(\"of\")) #insert \"of\" between the two nouns\n",
        "        #print(taggedWords)\n",
        "    idx += 1\n",
        "  print(\"Attributive Nouns:\", taggedWords)\n"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDJ0b90IagIm",
        "outputId": "212c33e7-9da5-4b42-8822-31d5342e3547"
      },
      "source": [
        "print(nltk.pos_tag(nltk.word_tokenize(\"a cat the cat\")))"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('a', 'DT'), ('cat', 'NN'), ('the', 'DT'), ('cat', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxFzmK0_a_wD"
      },
      "source": [
        "englishNegAndAff = [(\"no one\", \"someone\"), (\"nothing\", \"something\"), (\"none\", \"some\"), (\"neither\", \"also\"), (\"never\", \"always\"), (\"not\", \"\")] #no = exception. If no one --> someone. if no + noun --> none"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CStsjEEfJZW"
      },
      "source": [
        "spanishNegAndAff = [(\"nadie\", \"algien\"), (\"nada\", \"algo\"), (\"ningún\", \"algún\"), (\"ninguna\", \"alguna\"), (\"ningunas\", \"algunas\"), (\"ningunos\", \"algunos\"), (\"tampoco\", \"también\"), (\"nunca\", \"siempre\")] #exception = no"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRiyEVEOMikU"
      },
      "source": [
        "[nltk Acronym Meanings](https://www.guru99.com/pos-tagging-chunking-nltk.html#:~:text=POS%20Tagging%20(Parts%20of%20Speech,is%20also%20called%20grammatical%20tagging.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO9DQu_L_EvG"
      },
      "source": [
        "[Grammar Diff Btwn Spanish and English](https://www.thoughtco.com/grammatical-differences-between-spanish-and-english-4119326#:~:text=Word%20order%20is%20less%20fixed,subjunctive%20mood%20than%20English%20does.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA82S6sS5K_F"
      },
      "source": [
        "***Sometimes Pronouns Come Before Verbs and Nouns After***\n",
        "\n",
        "Lo escribió Cervantes.\n",
        "*   Megan wrote it (the book) --> It (the book) wrote Cervantes\n",
        "*   NNP VBD PRP --> PRP VBD NNP\n",
        "\n",
        "\"No recuerdo el momento en que salió Pablo\"\n",
        "*   I don't remember the moment in which Pablo left --> I don't remember the moment in which left Pablo\n",
        "*   (PRP VBP RB VB DT NN IN WDT) NNP VBD --> (same) VBD NNP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCp1Ijm678FV"
      },
      "source": [
        "***Spanish Uses Double Negatives***\n",
        "\n",
        "* Apenas come. (She barely eats.)\n",
        "* Apenas come nada. (She barely eats anything --> she barely eats nothing)\n",
        "* No tengo ninguno. (I don't have any --> I don't have none)\n",
        "* Nadie sabe eso. (Nobody knows that.)\n",
        "* Jamás fumo. (I never smoke.)\n",
        "* Tampoco comió. (She didn't eat either.)\n",
        "* Tampoco comió nada. (She didn't eat anything either --> She didn't eat nothing either)\n",
        "* No habló. (He didn't speak.)\n",
        "* No dijo nada. (He said nothing --> He didn't say nothing)\n",
        "* No le dijo nada a nadie. (He didn't say anything to anybody --> He didn't say nothing to nobody)\n",
        "* No compro ninguno. (I'm not buying any --> I'm not buying none)\n",
        "* Nunca le compra nada a nadie. (She never buys anything for anybody --> She never buy nothing for nobody)\n",
        "* No come ni siquiera pan. (He doesn't even eat bread --> He doesnt --honestly I don't even know how to translate this literally)\n",
        "* Ni siquiera come pan. (He doesn't even eat bread.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cNVR48c9tA0"
      },
      "source": [
        "***Can Omit Pronouns if Verb is Conjugated***\n",
        "\n",
        "* No compro ninguno. (I'm not buying any --> I'm not buying none)\n",
        "* Nunca le compra nada a nadie. (She never buys anything for anybody --> She never buy nothing for nobody)\n",
        "* No come ni siquiera pan. (He doesn't even eat bread --> He doesnt --honestly I don't even know how to translate this literally)\n",
        "* Ni siquiera come pan. (He doesn't even eat bread.)\n",
        "\n",
        "*Especially* Don't need \"I\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AASQ8qUr-lRI"
      },
      "source": [
        "***Attributive Nouns***\n",
        "Not coffee cup --> taza para cafe --> cup of coffee\n",
        "\n",
        "if two nouns next to each other --> switch nouns and add \"of\" (para/de) between\n",
        "\n",
        "* In some cases, this is accomplished by Spanish having adjectival forms that don't exist in English. For example, informático can be the equivalent of \"computer\" as an adjective, so a computer table is a mesa informática."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYO7dHT41p_s"
      },
      "source": [
        "***Tacking Pronouns to End of Verbs When \"To\"***\n",
        " * darmelo = give it to me --> give me it\n",
        " * escribamostelo = let's write it to you --> let's write you it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09tMWQsI1ogT"
      },
      "source": [
        "***bold text***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ll1gteCt_KZ"
      },
      "source": [
        "def toEnglishStyle(taggedWords):\n",
        "  print(\"under development\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4yH_HBz0BZf"
      },
      "source": [
        "### **English Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJG4occ7x8Tb",
        "outputId": "73e3bec1-fccd-4fa2-c8ba-a5fd3ca4fa68"
      },
      "source": [
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7iAKLsWdED2"
      },
      "source": [
        "**Syntax** **trees**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFvVZ0c9k2mE"
      },
      "source": [
        "import os\n",
        "import matplotlib as mpl\n",
        "if os.environ.get('DISPLAY','') == '':\n",
        "    print('no display found. Using non-interactive Agg backend')\n",
        "    mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzqAOhi6lQug",
        "outputId": "05c289e1-bac0-4d6e-ee76-b80c9f91fdc1"
      },
      "source": [
        "### CREATE VIRTUAL DISPLAY ###\n",
        "!apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
            "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
            "  404  Not Found [IP: 91.189.88.142 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_1.19.6-1ubuntu4.8_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_PeZhw_i_Mj",
        "outputId": "ef68648f-5bcd-4557-fc6b-81af0754775d"
      },
      "source": [
        "# EXTRA STUFF TO DISPLAY NLTK SYNTAX TREES #  \n",
        "%matplotlib inline\n",
        "### INSTALL GHOSTSCRIPT ) ###\n",
        "!apt install ghostscript python3-tk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ghostscript is already the newest version (9.26~dfsg+0-0ubuntu0.18.04.14).\n",
            "python3-tk is already the newest version (3.6.9-1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "lP1mbYPsfyfG",
        "outputId": "3d6d68ee-cdbd-41b2-d3ff-8bf4e5a74989"
      },
      "source": [
        "chunked_sentence = '(S (NP this tree) (VP (V is) (AdjP pretty)))'\n",
        "\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display\n",
        "tree = Tree.fromstring(str(chunked_sentence))\n",
        "display(tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: couldn't connect to display \":1.0\""
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', ['this', 'tree']), Tree('VP', [Tree('V', ['is']), Tree('AdjP', ['pretty'])])])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g-Y9HWMycIR",
        "outputId": "f9aa8886-5011-4ef6-b1ad-6520e070ae67"
      },
      "source": [
        "Sentence = \"The white dog ran\"\n",
        "tokenized = nltk.word_tokenize(Sentence)\n",
        "print(tokenized)\n",
        "tagged = nltk.pos_tag(tokenized)\n",
        "print(tagged)\n",
        "chunkGram = r\"\"\"Chunk: {<DT\\w?>*<JJ\\w?>}\"\"\" #creating a chunk\n",
        "chunkParser = nltk.RegexpParser(chunkGram) #parsing the chunk\n",
        "#chunked = chunkParser.parse(tagged)\n",
        "for tree in chunkParser.parse(tagged): #creates a syntax tree for the parsed chunk\n",
        "    print(tree)\n",
        "    #tree.draw()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'white', 'dog', 'ran']\n",
            "[('The', 'DT'), ('white', 'JJ'), ('dog', 'NN'), ('ran', 'VBD')]\n",
            "(Chunk The/DT white/JJ)\n",
            "('dog', 'NN')\n",
            "('ran', 'VBD')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n5otTnLbWYZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-95_ks94yZr",
        "outputId": "27a39db2-3585-4507-9c30-7db407cfbfee"
      },
      "source": [
        "Sentence = \"white the ran dog.\"\n",
        "tokenized = nltk.word_tokenize(Sentence)\n",
        "print(tokenized)\n",
        "tagged = nltk.pos_tag(tokenized)\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['white', 'the', 'ran', 'dog', '.']\n",
            "[('white', 'JJ'), ('the', 'DT'), ('ran', 'NN'), ('dog', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-kpFQz7J6uA"
      },
      "source": [
        "### **Spanish Data?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxr9qSXUSrAn",
        "outputId": "3f0fda82-d5fc-4030-ced0-3ddba92be74b"
      },
      "source": [
        "!git clone https://github.com/alvations/spaghetti-tagger.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'spaghetti-tagger' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6UhXZTIXcCE",
        "outputId": "40dc9b92-3d4f-4a92-ae71-c9865ab4b4e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/cont\n",
        "ent/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foieQg6LOart",
        "outputId": "5093cc11-6bbf-4cab-e8c2-b2736845af19"
      },
      "source": [
        "nltk.corpus.cess_esp.words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['El', 'grupo', 'estatal', 'Electricité_de_France', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tolSY-XOki1",
        "outputId": "9e544efc-2615-457a-b064-54455b4d4f54"
      },
      "source": [
        "nltk.corpus.cess_esp.sents()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdRSXGv6MLhA",
        "outputId": "4ef97d05-671d-46a3-ef90-b538bf0a33f3"
      },
      "source": [
        "nltk.download(\"cess_esp\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Package cess_esp is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRUUYWeU1MlA"
      },
      "source": [
        "from nltk.corpus import cess_esp as cess\n",
        "from nltk import UnigramTagger as ut\n",
        "from nltk import BigramTagger as bt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaDbFAtkOmKZ"
      },
      "source": [
        "# Read the corpus into a list, \n",
        "# each entry in the list is one sentence.\n",
        "cess_sents = cess.tagged_sents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBtEmt7FPZ8i"
      },
      "source": [
        "# Train the unigram tagger\n",
        "uni_tag = ut(cess_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEGYFga6P4No"
      },
      "source": [
        "spanishSentence = \"el perro blanco corrió\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_tyHxvV1zBY",
        "outputId": "924aa23b-eb22-45a0-d405-872daa8e5e11"
      },
      "source": [
        "#tokenized = spanishSentence.split()\n",
        "tokenized = nltk.word_tokenize(spanishSentence)\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)\n",
        "tagged2 = mytagger_uni.tag(tokenized)\n",
        "print(tagged2)\n",
        "tagged4 = mytagger_bi.tag(tokenized)\n",
        "print(tagged4)\n",
        "print(tagged4[0])\n",
        "print(tagged4[0][0]) #How to parce through intervals of the tuple in the array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['el', 'perro', 'blanco', 'corrió']\n",
            "[('el', 'da0ms0'), ('perro', 'ncms000'), ('blanco', 'aq0ms0'), ('corrió', 'vmis3s0')]\n",
            "[('el', 'da0ms0'), ('perro', 'ncms000'), ('blanco', 'aq0ms0'), ('corrió', 'vmis3s0')]\n",
            "[('el', 'da0ms0'), ('perro', 'ncms000'), ('blanco', 'aq0ms0'), ('corrió', 'vmis3s0')]\n",
            "('el', 'da0ms0')\n",
            "el\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEfuWamYEmjp",
        "outputId": "dca0d943-34b2-4067-8b7f-83771dfce414"
      },
      "source": [
        "femSentence = \"la niña es fea\"\n",
        "tokenized1 = femSentence.split()\n",
        "print(tokenized1)\n",
        "tagged3 = sp.pos_tag(tokenized1)\n",
        "print(tagged3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['la', 'niña', 'es', 'fea']\n",
            "[('la', 'da0fs0'), ('niña', 'ncfs000'), ('es', 'vsip3s0'), ('fea', 'aq0fs0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUsKWxc9Pvs7",
        "outputId": "3597b30d-3846-44ee-ab13-3a9b2a95d7e0"
      },
      "source": [
        "# Split corpus into training and testing set.\n",
        "train = int(len(cess_sents)*90/100) # 90%\n",
        "\n",
        "# Train a bigram tagger with only training data.\n",
        "bi_tag = bt(cess_sents[:train], backoff=uni_tag)\n",
        "\n",
        "# Evaluates on testing data remaining 10%\n",
        "bi_tag.evaluate(cess_sents[train+1:])\n",
        "\n",
        "# Using the tagger.\n",
        "bi_tag.tag(Sentence.split(\" \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('el', 'da0ms0'),\n",
              " ('perro', 'ncms000'),\n",
              " ('blanco', 'aq0ms0'),\n",
              " ('corrió', 'vmis3s0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "G4lJo9d67yBw",
        "outputId": "e502f828-e55b-438a-e084-375dd60b8e0b"
      },
      "source": [
        "nltk.corpus.genesis.tagged_words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-673971ba013a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PlaintextCorpusReader' object has no attribute 'tagged_words'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0AR8EKfCHbP"
      },
      "source": [
        "# **Sanskrit Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1IRb82_D-Cd"
      },
      "source": [
        "Mounting Drive and importing text files of Sanskrit texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvEDZkqnEK-R",
        "outputId": "cebce4f4-ec09-4855-8b7d-315cfff243fc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqsUhQPYCObp"
      },
      "source": [
        "gItA = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/bhagavadgItA.txt\",\"r\")\n",
        "meghadhUta = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/meghadUta.txt\",\"r\")\n",
        "rAmAyaNa = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/rAmAyaNa.txt\", \"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "snXpa2v0F_9D",
        "outputId": "2b15ed1b-3c47-4e86-f73d-67b8d43ec2f4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-b5768a72344c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pos/hmm/make -f makefile-osx\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv8b6SgbHmJa"
      },
      "source": [
        "comment to delete later: trying to figure out how to use repository here : https://github.com/ad2476/pos-research which has a sanskrit tagger. "
      ]
    }
  ]
}