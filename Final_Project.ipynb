{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project",
      "provenance": [],
      "collapsed_sections": [
        "K0AR8EKfCHbP"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meganificent/Language-Style-Transfer/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ths7y9Z4OYNM"
      },
      "source": [
        "# Spanglish Machine Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G944WiS3dM82"
      },
      "source": [
        "## Stuff to Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drQ3Elk78cHp"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOSsb-duk_Ot",
        "outputId": "c819c784-7996-40bc-eb91-08a2b864b72e"
      },
      "source": [
        "import spaghetti as sp\n",
        "import nltk\n",
        "nltk.download('cess_esp')\n",
        "mytagger = sp.CESSTagger()\n",
        "mytagger_uni = mytagger.uni\n",
        "mytagger_bi = mytagger.bi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "*** First-time use of cess tagger ***\n",
            "Training tagger ...\n",
            "Tagger trained with cess using UnigramTagger and BigramTagger.\n",
            "Tagger trained with cess_nomwe using UnigramTagger and BigramTagger.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gupDVPEVCNXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576e90f1-287a-43fe-e392-e5c179d19d7c"
      },
      "source": [
        "!pip install googletrans==3.1.0a0\n",
        "import googletrans\n",
        "print(googletrans.LANGUAGES)\n",
        "from googletrans import Translator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.8MB/s \n",
            "\u001b[?25hCollecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 6.2MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.6MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=89bba2629bf2c8a7243441cd91ebfa75c130da152242fdd1be79e172b55418e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h2, sniffio, h11, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.4.0 sniffio-1.2.0\n",
            "{'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'he': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'or': 'odia', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'ug': 'uyghur', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMZv6MloFg0y"
      },
      "source": [
        "translator = Translator(service_urls=['translate.googleapis.com'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kaAWdpZBqzq"
      },
      "source": [
        "## Spanglish Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60JXLI0rRHRD"
      },
      "source": [
        "def spanglishMachine(string): \n",
        "  origTokenized = nltk.word_tokenize(string)\n",
        "  translatedArray = \"\"\n",
        "  translatedString = \"\"\n",
        "  originalString = \"\"\n",
        "\n",
        "  #Use Google Translate to determine original language of input and feed input into either toSpanishStyle() or toEnglishStyle()\n",
        "  \n",
        "  translated = translator.translate(string, dest = \"en\") #initially translate to english\n",
        "  if translated.src == \"en\": #if the original language was english\n",
        "    tagged = nltk.pos_tag(origTokenized) #tag POS using nltk\n",
        "    #originalString = \" \".join([word[0] for word in tagged]) #sentence for testing\n",
        "    #print(\"Original:\", tagged)\n",
        "\n",
        "    translatedArray = toSpanishStyle(tagged) #transfer to Spanish\n",
        "    #translatedString = \" \".join([word[0] for word in translatedArray]) #sentence for testing\n",
        "    \n",
        "\n",
        "  else: #if original language was Spanish\n",
        "    tagged = sp.pos_tag(origTokenized) #tag POS using spaghetti\n",
        "    #originalString = \" \".join([word[0] for word in tagged]) #sentence for testing\n",
        "    #print(\"Original:\", tagged)\n",
        "    translatedArray = changeSpanTags(toEnglishStyle(tagged)) #transfer to English\n",
        "    #translatedString = \" \".join([word[0] for word in translatedArray])#sentence for testing\n",
        "\n",
        "    #print(\"after:\", translatedString)\n",
        "    #print(\"after with tags:\", translatedArray)\n",
        "    #print(\"before:\", originalString)\n",
        "\n",
        "  return translatedArray"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSvhwFvaQEZL"
      },
      "source": [
        "## *English --> Spanish*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFT8U7D8dQAW"
      },
      "source": [
        "spanglishMachine(\"I do not approve of Anthony's tardiness\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wB6cpEPmoc4",
        "outputId": "fa6a62cc-6b4f-459b-9e83-54e1f16d94e2"
      },
      "source": [
        "print(nltk.pos_tag(nltk.word_tokenize(\"my sister's day is not going well\")))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('my', 'PRP$'), ('sister', 'NN'), (\"'s\", 'POS'), ('day', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('going', 'VBG'), ('well', 'RB')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWvj4Ik8lHnw",
        "outputId": "21b47d08-3ba0-4c71-b5a7-34deaecf3e68"
      },
      "source": [
        "spanglishMachine(\"My sister's day isn't going well\")"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "owned noun start:  3 owned noun end:  4\n",
            "[('My', 'PRP$'), ('sister', 'NN'), (\"'s\", 'POS'), ('day', 'NN'), ('of', 'IN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
            "[('My', 'PRP$'), ('sister', 'NN'), (\"'s\", 'POS'), ('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
            " 1 - Posessive Nouns pt 1: [('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
            "2 - Long Sentence Verb+Noun: [('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
            "Remove 'To' and Swap PRP: [('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
            "Remove Pronouns Before Verbs: [('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
            "Pronoun+Verb+Noun: [('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
            "6 - Adjectives after Nouns: [('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
            "7 - Add 'the': [('the', 'DT'), ('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
            "8 - Attributive Nouns: [('the', 'DT'), ('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
            "9 - Move not after verbs: [('the', 'DT'), ('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), ('not', 'RB'), ('is', 'VBZ'), ('going', 'VBG'), ('well', 'RB')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 'DT'),\n",
              " ('day', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('My', 'PRP$'),\n",
              " ('sister', 'NN'),\n",
              " ('not', 'RB'),\n",
              " ('is', 'VBZ'),\n",
              " ('going', 'VBG'),\n",
              " ('well', 'RB')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsCV7YOV6vmV",
        "outputId": "bbf4795b-9c53-4f14-ef76-4f297411dd25"
      },
      "source": [
        "hm = [('the', 'DT'), ('day', 'NN'), ('of', 'IN'), ('My', 'PRP$'), ('sister', 'NN'), (\"n't\", 'RB'), ('not', 'RB'), ('going', 'VBG'), ('well', 'RB')]\n",
        "iIndices = [(\"not\", \"RB\") for i in hm if (i == (\"n't\", \"RB\")) | (i == (\"not\", \"RB\"))]\n",
        "print(iIndices)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('not', 'RB'), ('not', 'RB')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqcAIiut2OT"
      },
      "source": [
        "def toSpanishStyle(taggedWords):\n",
        "\n",
        "  #Rule 1: Move Possessive Nouns After What they Possess\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords)-1:\n",
        "    if taggedWords[idx][1] == \"POS\": #if current word is a possessive ending (\"'s\")\n",
        "      #possessive + noun and all its qualifiers\n",
        "      posNounEnd = idx-1\n",
        "      posNounStart = getNounStart(posNounEnd, taggedWords)\n",
        "      ownedNounStart = idx+1\n",
        "      ownedNounEnd = idx+2 #added 1 for the sake of indexing later\n",
        "      if idx+2 < len(taggedWords): #avoid runtime errors\n",
        "        if taggedWords[idx+2][1][:2] == \"NN\":\n",
        "          print(taggedWords[idx+2])\n",
        "          ownedNounEnd = idx+3\n",
        "          if idx+3 < len(taggedWords): #avoid runtime errors\n",
        "            if taggedWords[idx+3][1][:2] == \"NN\":\n",
        "              print(taggedWords[idx+3])\n",
        "              ownedNounEnd = idx+4\n",
        "      #move posNoun after ownedNoun\n",
        "      taggedWords[ownedNounEnd:ownedNounEnd] = nltk.pos_tag(nltk.word_tokenize(\"of\")) #insert \"of\" after owned noun\n",
        "      taggedWords[ownedNounEnd+1:ownedNounEnd+1] = taggedWords[posNounStart:posNounEnd+1] #add posessive noun stuff (minus 's) after owned noun stuff's \"of\"\n",
        "      del taggedWords[posNounStart:posNounEnd+2] #remove original pos noun stuff and pos\n",
        "\n",
        "      idx+=1 #so dont recount things\n",
        "    idx +=1\n",
        "  print(\" 1 - Posessive Nouns pt 1:\", taggedWords)\n",
        "\n",
        "  #Rule 2: Noun after Verb When It's a Verb applied to the second noun in the Sentence (because switch more common with long sentences)\n",
        "  idx = 0\n",
        "  numSubjects = 0\n",
        "  while idx < len(taggedWords)-1:\n",
        "    if (taggedWords[idx][1][:2] == \"NN\") | (taggedWords[idx][1] == \"PRP\"): #if current word is a noun\n",
        "      if taggedWords[idx+1][1][:2] == \"VB\":        \n",
        "        numSubjects += 1\n",
        "        if numSubjects == 2: #if currently on second noun with verb\n",
        "          nounEnd = idx\n",
        "          nounStart = getNounStart(nounEnd, taggedWords) #where noun's qualifier's start\n",
        "          taggedWords[idx+1:idx+1] = taggedWords[nounStart:nounEnd+1] #add noun stuff after verb\n",
        "          del taggedWords[nounStart:nounEnd+1] #remove ealier noun elements\n",
        "    \n",
        "    idx += 1\n",
        "  print(\"2 - Long Sentence Verb+Noun:\", taggedWords)\n",
        "\n",
        "\n",
        "  #Rule 3: verb + pronoun1 + preposition + pronoun2 --> pronoun 2 + pronoun 1 + verb\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 3:\n",
        "    if taggedWords[idx][1][:2] == \"VB\": #if current word = verb\n",
        "      if (taggedWords[idx+1][1] == \"PRP\") & ((taggedWords[idx+2][1] == \"IN\") | (taggedWords[idx+2][1] == \"TO\")) & (taggedWords[idx+3][1] == \"PRP\"): #if PRP + IN + PRP\n",
        "        taggedWords[idx+1], taggedWords[idx+3] = taggedWords[idx+3], taggedWords[idx+1] #switch both pronouns\n",
        "        del taggedWords[idx+2] #delete IN\n",
        "        taggedWords[idx:idx] = taggedWords[idx+1:idx+3] #move both pronouns before the verb\n",
        "        del taggedWords[idx+3:idx+5] #delete the pronouns after the verb\n",
        "        idx+=1\n",
        "    idx+=1\n",
        "  print(\"3 - Remove 'To' and Swap PRP:\", taggedWords)\n",
        "\n",
        "  #Rule 4: Remove Pronouns Directly Before Conjugated Verbs If not 2 or 3 pronoun group before verb\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1:\n",
        "    if taggedWords[idx][1] == \"PRP\": #if current word is a pronoun\n",
        "      if taggedWords[idx+1][1][:2] == \"VB\": #if word after is a verb\n",
        "        if idx-1 >= 0:\n",
        "          if taggedWords[idx-1][1] != \"PRP\": #if word before curerent word is not a pronoun\n",
        "            del taggedWords[idx] #remove the pronoun\n",
        "        else:\n",
        "          del taggedWords[idx] #if there is nothing before the pronoun at all, delete the pronoun\n",
        "    idx += 1\n",
        "  print(\"4 - Remove Pronouns Before Verbs:\", taggedWords)\n",
        "\n",
        "  #Rule 5: Noun + Verb + Pronoun --> Pronoun + Verb + Noun\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1:\n",
        "    if taggedWords[idx][1][:2] == \"VB\": #if current word is a verb\n",
        "      #print(idx)\n",
        "      if taggedWords[idx+1][1] == \"PRP\": #if there's a pronoun directly after the verb\n",
        "        if idx-1 >= 0: #to avoid runtime errors\n",
        "          if taggedWords[idx-1][1][:2] == \"NN\": #if there's a noun right before the verb\n",
        "            nounEnd = idx # idx-1 + 1 for the sake of indexing later\n",
        "            nounStart = getNounStart(idx-1, taggedWords) #where noun's qualifier's start\n",
        "            if idx+2 < len(taggedWords):\n",
        "              if taggedWords[idx+2] == \"PRP\": #if there's a pronoun verb directly after it\n",
        "                taggedWords[idx-1] = taggedWords[idx+1:idx+3]\n",
        "                del taggedWords[idx+1:idx+3] #more both pronouns before verb\n",
        "              else:\n",
        "                taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx] #swap pronoun and verb positions\n",
        "            else:\n",
        "              taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx] #swap pronoun and verb positions\n",
        "            taggedWords[idx+2:idx+2] = taggedWords[nounStart:nounEnd] #add noun stuff after where pronoun used to be\n",
        "            del taggedWords[nounStart:nounEnd] #remove ealier noun elements\n",
        "            idx +=1 #so I don't recount verb\n",
        "    idx +=1\n",
        "  print(\"Pronoun+Verb+Noun:\", taggedWords)\n",
        "\n",
        "  #Rule 6: Adjective + Noun --> Noun + Adjective\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1: #for every tuple (word) in Array\n",
        "    if taggedWords[idx][1][:2] == \"JJ\": #if current word is an adjective (can tell general tag by first 2 letters of acronym)\n",
        "      if taggedWords[idx+1][1][:2] == \"NN\": #if the proceding word is a noun #how do I avoid an error if adjective is last word in list?\n",
        "        taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx]\n",
        "        #idx += 1\n",
        "    idx += 1\n",
        "  print(\"6 - Adjectives after Nouns:\", taggedWords)\n",
        "\n",
        "\n",
        "  #Rule 7: \"The\" Before Every Non-Proper Noun\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords):\n",
        "    if (taggedWords[idx][1][:2] == \"NN\") & (taggedWords[idx][1][:3] != \"NNP\"): #if word is a noun and not a proper noun\n",
        "      if idx == 0:\n",
        "        taggedWords[idx:idx] = nltk.pos_tag(nltk.word_tokenize(\"the\"))\n",
        "        idx += 1 #so we don't recount nouns\n",
        "      else: #to avoid runtime errors\n",
        "        if (taggedWords[idx - 1][1] != \"DT\") & (taggedWords[idx - 1][1] != \"PRP$\") & (taggedWords[idx - 1][1][:2] != \"NN\"): #if noun doesn't already have an \"a\" or \"the\" in front of it and if it doesn't have another noun in front of it or a posessive pronouns like \"su\", waiting to be turned into noun+of+noun\n",
        "          taggedWords[idx:idx] = nltk.pos_tag(nltk.word_tokenize(\"the\"))\n",
        "          idx += 1 #so we don't recount nouns\n",
        "    idx += 1\n",
        "  print(\"7 - Add 'the':\", taggedWords)\n",
        "\n",
        "\n",
        "  #Rule 8: Attributive Nouns (coffee cup --> cup of coffee)\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1:\n",
        "    if (taggedWords[idx][1][:2] == \"NN\") & (taggedWords[idx+1][1][:2] == \"NN\"): #if 2 nouns next to each other\n",
        "        taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx] #swap the nouns\n",
        "        #print(taggedWords)\n",
        "        taggedWords[idx+1:idx+1] = nltk.pos_tag(nltk.word_tokenize(\"of\")) #insert \"of\" between the two nouns\n",
        "        #print(taggedWords)\n",
        "    idx += 1\n",
        "  print(\"8 - Attributive Nouns:\", taggedWords)\n",
        "\n",
        "  #Rule 9: If \"not\" or \"n't\" after verb, move \"not\" to before verb\n",
        "  nots = [i for i in taggedWords if (i == (\"n't\", \"RB\")) | (i == (\"not\", \"RB\"))] #array of all nots and n'ts\n",
        "  idx = 0\n",
        "  for i in nots:\n",
        "    idx = taggedWords.index(i, idx+1) #find index of tuple starting at next slot\n",
        "    if taggedWords[idx-1][1][:2] == \"VB\": #if the next word is a verb\n",
        "      taggedWords[idx] = (\"not\", \"RB\") #replace n't with not\n",
        "      taggedWords[idx], taggedWords[idx-1] = taggedWords[idx-1], taggedWords[idx] #switch verb and not\n",
        "  print(\"9 - Move not after verbs:\", taggedWords)\n",
        "\n",
        "  #Rule 10: VB + IN/TO + PRP --> PRP + VB\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords)-2:\n",
        "    if (taggedWords[idx][1][:2] == \"VB\") & ((taggedWords[idx+1][1] == \"IN\") | (taggedWords[idx+1][1] == \"TO\")) & (taggedWords[idx+2][1] == \"PRP\"): #VB + IN/TO + PRP\n",
        "      taggedWords[idx], taggedWords[idx+2] = taggedWords[idx+2], taggedWords[idx] #swap verb and pronoun\n",
        "      del taggedWords[idx+1] #delete IN\n",
        "      idx +=1\n",
        "    idx+= 1\n",
        "\n",
        "  print(\"10 - TO/IN + PRP\", taggedWords)\n",
        "\n",
        "\n",
        "  return(taggedWords)\n"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ24BxdFuaLg"
      },
      "source": [
        "def getNounStart(endIndex, wordsArray):\n",
        "  nounStart = endIndex\n",
        "  while True:\n",
        "    if endIndex-1 >= 0: #to avoid runtime errors\n",
        "      if (wordsArray[endIndex-1][1][:2] == \"DT\") | (wordsArray[endIndex-1][1] == \"PRP$\") | (wordsArray[endIndex-1][1][:2] == \"POS\") | (wordsArray[endIndex-1][1][:2] == \"JJ\") | (wordsArray[endIndex-1][1][:2] == \"NN\"): #if there's a determiner or adjective next to the noun\n",
        "        nounStart = endIndex-1\n",
        "        if endIndex-2 >= 0: #to avoid runtime errors\n",
        "          if (wordsArray[endIndex-2][1][:2] == \"DT\") | (wordsArray[endIndex-2][1][:2] == \"PRP$\") | (wordsArray[endIndex-2][1][:2] == \"JJ\") | (wordsArray[endIndex-2][1][:2] == \"NN\"): #if there's a determiner next to the adjective\n",
        "            nounStart = endIndex-2\n",
        "            if endIndex-3 >=0:\n",
        "              if (wordsArray[endIndex-3][1][:2] == \"DT\") | (wordsArray[endIndex-3][1][:2] == \"PRP$\") | (wordsArray[endIndex-3][1][:2] == \"JJ\"): #if there's a determiner, pronoun, or possessive pronoun\n",
        "                nounStart = endIndex-3\n",
        "                if endIndex-4 >= 0:\n",
        "                  if (wordsArray[endIndex-4][1][:2] == \"DT\") | (wordsArray[endIndex-4][1][:2] == \"PRP$\"): #if there's a determiner or possessive pronoun\n",
        "                    nounStart = endIndex-4\n",
        "    \n",
        "    if nounStart-1 >= 0: #to avoid runtime errors\n",
        "      if wordsArray[nounStart-1][0] == \"of\":\n",
        "        endIndex = nounStart-1 #where noun's qualifier's start\n",
        "      else:\n",
        "        break\n",
        "    else:\n",
        "      break\n",
        "    print(wordsArray[nounStart:endIndex+1])\n",
        "  return nounStart"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne9tmi-0P7I2"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nZJ6VMCwta7",
        "outputId": "edabc1d1-49d9-4591-e6f2-1a4fc688480d"
      },
      "source": [
        "spanglishMachine(\"white coffee cup spilled him\")"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1 - Posessive Nouns pt 1: [('white', 'JJ'), ('coffee', 'NN'), ('cup', 'NN'), ('spilled', 'VBD'), ('him', 'PRP')]\n",
            "2 - Long Sentence Verb+Noun: [('white', 'JJ'), ('coffee', 'NN'), ('cup', 'NN'), ('spilled', 'VBD'), ('him', 'PRP')]\n",
            "Remove 'To' and Swap PRP: [('white', 'JJ'), ('coffee', 'NN'), ('cup', 'NN'), ('spilled', 'VBD'), ('him', 'PRP')]\n",
            "Remove Pronouns Before Verbs: [('white', 'JJ'), ('coffee', 'NN'), ('cup', 'NN'), ('spilled', 'VBD'), ('him', 'PRP')]\n",
            "Pronoun+Verb+Noun: [('him', 'PRP'), ('spilled', 'VBD'), ('white', 'JJ'), ('coffee', 'NN'), ('cup', 'NN')]\n",
            "6 - Adjectives after Nouns: [('him', 'PRP'), ('spilled', 'VBD'), ('coffee', 'NN'), ('cup', 'NN'), ('white', 'JJ')]\n",
            "7 - Add 'the': [('him', 'PRP'), ('spilled', 'VBD'), ('the', 'DT'), ('coffee', 'NN'), ('cup', 'NN'), ('white', 'JJ')]\n",
            "8 - Attributive Nouns: [('him', 'PRP'), ('spilled', 'VBD'), ('the', 'DT'), ('cup', 'NN'), ('of', 'IN'), ('coffee', 'NN'), ('white', 'JJ')]\n",
            "9 - Move not after verbs: [('him', 'PRP'), ('spilled', 'VBD'), ('the', 'DT'), ('cup', 'NN'), ('of', 'IN'), ('coffee', 'NN'), ('white', 'JJ')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('him', 'PRP'),\n",
              " ('spilled', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('cup', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('coffee', 'NN'),\n",
              " ('white', 'JJ')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnV9pAZlq1Mc",
        "outputId": "4ff3dace-18d9-4117-b3f0-37c427aa4d75"
      },
      "source": [
        "spanglishMachine(\"Megan wrote it\")"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1 - Posessive Nouns pt 1: [('Megan', 'NNP'), ('wrote', 'VBD'), ('it', 'PRP')]\n",
            "2 - Long Sentence Verb+Noun: [('Megan', 'NNP'), ('wrote', 'VBD'), ('it', 'PRP')]\n",
            "Remove 'To' and Swap PRP: [('Megan', 'NNP'), ('wrote', 'VBD'), ('it', 'PRP')]\n",
            "Remove Pronouns Before Verbs: [('Megan', 'NNP'), ('wrote', 'VBD'), ('it', 'PRP')]\n",
            "Pronoun+Verb+Noun: [('it', 'PRP'), ('wrote', 'VBD'), ('Megan', 'NNP')]\n",
            "6 - Adjectives after Nouns: [('it', 'PRP'), ('wrote', 'VBD'), ('Megan', 'NNP')]\n",
            "7 - Add 'the': [('it', 'PRP'), ('wrote', 'VBD'), ('Megan', 'NNP')]\n",
            "8 - Attributive Nouns: [('it', 'PRP'), ('wrote', 'VBD'), ('Megan', 'NNP')]\n",
            "9 - Move not after verbs: [('it', 'PRP'), ('wrote', 'VBD'), ('Megan', 'NNP')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('it', 'PRP'), ('wrote', 'VBD'), ('Megan', 'NNP')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxtwzGXLwz9K",
        "outputId": "cf8039b1-8d36-4b66-b052-73b1d2133171"
      },
      "source": [
        "spanglishMachine(\"I write but the hot sun stays down\")"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1 - Posessive Nouns pt 1: [('I', 'PRP'), ('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('hot', 'JJ'), ('sun', 'NN'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "2 - Long Sentence Verb+Noun: [('I', 'PRP'), ('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('hot', 'JJ'), ('sun', 'NN'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "Remove 'To' and Swap PRP: [('I', 'PRP'), ('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('hot', 'JJ'), ('sun', 'NN'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "Remove Pronouns Before Verbs: [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('hot', 'JJ'), ('sun', 'NN'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "Pronoun+Verb+Noun: [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('hot', 'JJ'), ('sun', 'NN'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "6 - Adjectives after Nouns: [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('sun', 'NN'), ('hot', 'JJ'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "7 - Add 'the': [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('sun', 'NN'), ('hot', 'JJ'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "8 - Attributive Nouns: [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('sun', 'NN'), ('hot', 'JJ'), ('stays', 'VBZ'), ('down', 'RP')]\n",
            "9 - Move not after verbs: [('write', 'VBP'), ('but', 'CC'), ('the', 'DT'), ('sun', 'NN'), ('hot', 'JJ'), ('stays', 'VBZ'), ('down', 'RP')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('write', 'VBP'),\n",
              " ('but', 'CC'),\n",
              " ('the', 'DT'),\n",
              " ('sun', 'NN'),\n",
              " ('hot', 'JJ'),\n",
              " ('stays', 'VBZ'),\n",
              " ('down', 'RP')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI0PEOKPUDmr",
        "outputId": "2d8cfb0b-1f93-4c07-9277-e10e8bb204db"
      },
      "source": [
        "spanglishMachine(\"I wrote while the blue wolf cried to the moon\")"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1 - Posessive Nouns pt 1: [('I', 'PRP'), ('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('blue', 'JJ'), ('wolf', 'NN'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "2 - Long Sentence Verb+Noun: [('I', 'PRP'), ('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('blue', 'JJ'), ('wolf', 'NN'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "Remove 'To' and Swap PRP: [('I', 'PRP'), ('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('blue', 'JJ'), ('wolf', 'NN'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "Remove Pronouns Before Verbs: [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('blue', 'JJ'), ('wolf', 'NN'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "Pronoun+Verb+Noun: [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('blue', 'JJ'), ('wolf', 'NN'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "6 - Adjectives after Nouns: [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('wolf', 'NN'), ('blue', 'JJ'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "7 - Add 'the': [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('wolf', 'NN'), ('blue', 'JJ'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "8 - Attributive Nouns: [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('wolf', 'NN'), ('blue', 'JJ'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "9 - Move not after verbs: [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('wolf', 'NN'), ('blue', 'JJ'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('wrote', 'VBD'),\n",
              " ('while', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('wolf', 'NN'),\n",
              " ('blue', 'JJ'),\n",
              " ('cried', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('the', 'DT'),\n",
              " ('moon', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XZXbCINXg6a",
        "outputId": "e19894ed-b18c-41c0-ecff-504bfd71e6ae"
      },
      "source": [
        "spanglishMachine(\"I wrote while blue coffee cups cried to the moon\")"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1 - Posessive Nouns pt 1: [('I', 'PRP'), ('wrote', 'VBD'), ('while', 'IN'), ('blue', 'JJ'), ('coffee', 'NN'), ('cups', 'NNS'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "2 - Long Sentence Verb+Noun: [('I', 'PRP'), ('wrote', 'VBD'), ('while', 'IN'), ('blue', 'JJ'), ('coffee', 'NN'), ('cups', 'NNS'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "Remove 'To' and Swap PRP: [('I', 'PRP'), ('wrote', 'VBD'), ('while', 'IN'), ('blue', 'JJ'), ('coffee', 'NN'), ('cups', 'NNS'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "Remove Pronouns Before Verbs: [('wrote', 'VBD'), ('while', 'IN'), ('blue', 'JJ'), ('coffee', 'NN'), ('cups', 'NNS'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "Pronoun+Verb+Noun: [('wrote', 'VBD'), ('while', 'IN'), ('blue', 'JJ'), ('coffee', 'NN'), ('cups', 'NNS'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "6 - Adjectives after Nouns: [('wrote', 'VBD'), ('while', 'IN'), ('coffee', 'NN'), ('cups', 'NNS'), ('blue', 'JJ'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "7 - Add 'the': [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('coffee', 'NN'), ('cups', 'NNS'), ('blue', 'JJ'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "8 - Attributive Nouns: [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('cups', 'NNS'), ('of', 'IN'), ('coffee', 'NN'), ('blue', 'JJ'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n",
            "9 - Move not after verbs: [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('cups', 'NNS'), ('of', 'IN'), ('coffee', 'NN'), ('blue', 'JJ'), ('cried', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('wrote', 'VBD'),\n",
              " ('while', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('cups', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('coffee', 'NN'),\n",
              " ('blue', 'JJ'),\n",
              " ('cried', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('the', 'DT'),\n",
              " ('moon', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TdJQd4o_tw3",
        "outputId": "43dfed80-4baf-459f-bce2-c6ba1d78bba7"
      },
      "source": [
        "spanglishMachine(\"blue sister's heart cried\")"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "owned noun start:  3 owned noun end:  4\n",
            "[('blue', 'JJ'), ('sister', 'NN'), (\"'s\", 'POS'), ('heart', 'NN'), ('of', 'IN'), ('cried', 'VBD')]\n",
            "[('blue', 'JJ'), ('sister', 'NN'), (\"'s\", 'POS'), ('heart', 'NN'), ('of', 'IN'), ('blue', 'JJ'), ('sister', 'NN'), ('cried', 'VBD')]\n",
            " 1 - Posessive Nouns pt 1: [('heart', 'NN'), ('of', 'IN'), ('blue', 'JJ'), ('sister', 'NN'), ('cried', 'VBD')]\n",
            "2 - Long Sentence Verb+Noun: [('heart', 'NN'), ('of', 'IN'), ('blue', 'JJ'), ('sister', 'NN'), ('cried', 'VBD')]\n",
            "Remove 'To' and Swap PRP: [('heart', 'NN'), ('of', 'IN'), ('blue', 'JJ'), ('sister', 'NN'), ('cried', 'VBD')]\n",
            "Remove Pronouns Before Verbs: [('heart', 'NN'), ('of', 'IN'), ('blue', 'JJ'), ('sister', 'NN'), ('cried', 'VBD')]\n",
            "Pronoun+Verb+Noun: [('heart', 'NN'), ('of', 'IN'), ('blue', 'JJ'), ('sister', 'NN'), ('cried', 'VBD')]\n",
            "6 - Adjectives after Nouns: [('heart', 'NN'), ('of', 'IN'), ('sister', 'NN'), ('blue', 'JJ'), ('cried', 'VBD')]\n",
            "7 - Add 'the': [('the', 'DT'), ('heart', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sister', 'NN'), ('blue', 'JJ'), ('cried', 'VBD')]\n",
            "8 - Attributive Nouns: [('the', 'DT'), ('heart', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sister', 'NN'), ('blue', 'JJ'), ('cried', 'VBD')]\n",
            "9 - Move not after verbs: [('the', 'DT'), ('heart', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sister', 'NN'), ('blue', 'JJ'), ('cried', 'VBD')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 'DT'),\n",
              " ('heart', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('sister', 'NN'),\n",
              " ('blue', 'JJ'),\n",
              " ('cried', 'VBD')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie8Rxo-oBvmW",
        "outputId": "40f2d072-3c86-483d-8a68-a65549951fac"
      },
      "source": [
        "spanglishMachine(\"I wrote while Joanna's blue coffee cups cried to it\")"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('coffee', 'NN')\n",
            "('cups', 'NNS')\n",
            " 1 - Posessive Nouns pt 1: [('I', 'PRP'), ('wrote', 'VBD'), ('while', 'IN'), ('blue', 'JJ'), ('coffee', 'NN'), ('cups', 'NNS'), ('of', 'IN'), ('Joanna', 'NNP'), ('cried', 'VBD'), ('to', 'TO'), ('it', 'PRP')]\n",
            "[]\n",
            "2 - Long Sentence Verb+Noun: [('I', 'PRP'), ('wrote', 'VBD'), ('while', 'IN'), ('blue', 'JJ'), ('coffee', 'NN'), ('cups', 'NNS'), ('of', 'IN'), ('Joanna', 'NNP'), ('cried', 'VBD'), ('to', 'TO'), ('it', 'PRP')]\n",
            "3 - Remove 'To' and Swap PRP: [('I', 'PRP'), ('wrote', 'VBD'), ('while', 'IN'), ('blue', 'JJ'), ('coffee', 'NN'), ('cups', 'NNS'), ('of', 'IN'), ('Joanna', 'NNP'), ('cried', 'VBD'), ('to', 'TO'), ('it', 'PRP')]\n",
            "4 - Remove Pronouns Before Verbs: [('wrote', 'VBD'), ('while', 'IN'), ('blue', 'JJ'), ('coffee', 'NN'), ('cups', 'NNS'), ('of', 'IN'), ('Joanna', 'NNP'), ('cried', 'VBD'), ('to', 'TO'), ('it', 'PRP')]\n",
            "Pronoun+Verb+Noun: [('wrote', 'VBD'), ('while', 'IN'), ('blue', 'JJ'), ('coffee', 'NN'), ('cups', 'NNS'), ('of', 'IN'), ('Joanna', 'NNP'), ('cried', 'VBD'), ('to', 'TO'), ('it', 'PRP')]\n",
            "6 - Adjectives after Nouns: [('wrote', 'VBD'), ('while', 'IN'), ('coffee', 'NN'), ('cups', 'NNS'), ('blue', 'JJ'), ('of', 'IN'), ('Joanna', 'NNP'), ('cried', 'VBD'), ('to', 'TO'), ('it', 'PRP')]\n",
            "7 - Add 'the': [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('coffee', 'NN'), ('cups', 'NNS'), ('blue', 'JJ'), ('of', 'IN'), ('Joanna', 'NNP'), ('cried', 'VBD'), ('to', 'TO'), ('it', 'PRP')]\n",
            "8 - Attributive Nouns: [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('cups', 'NNS'), ('of', 'IN'), ('coffee', 'NN'), ('blue', 'JJ'), ('of', 'IN'), ('Joanna', 'NNP'), ('cried', 'VBD'), ('to', 'TO'), ('it', 'PRP')]\n",
            "9 - Move not after verbs: [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('cups', 'NNS'), ('of', 'IN'), ('coffee', 'NN'), ('blue', 'JJ'), ('of', 'IN'), ('Joanna', 'NNP'), ('cried', 'VBD'), ('to', 'TO'), ('it', 'PRP')]\n",
            "10 - TO/IN + PRP [('wrote', 'VBD'), ('while', 'IN'), ('the', 'DT'), ('cups', 'NNS'), ('of', 'IN'), ('coffee', 'NN'), ('blue', 'JJ'), ('of', 'IN'), ('Joanna', 'NNP'), ('it', 'PRP'), ('cried', 'VBD')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('wrote', 'VBD'),\n",
              " ('while', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('cups', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('coffee', 'NN'),\n",
              " ('blue', 'JJ'),\n",
              " ('of', 'IN'),\n",
              " ('Joanna', 'NNP'),\n",
              " ('it', 'PRP'),\n",
              " ('cried', 'VBD')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ads14dBgYnn",
        "outputId": "374644ef-4429-4cea-c075-6ba823b2fcd9"
      },
      "source": [
        "spanglishMachine(\"she wrote it to him\")"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1 - Posessive Nouns pt 1: [('she', 'PRP'), ('wrote', 'VBD'), ('it', 'PRP'), ('to', 'TO'), ('him', 'PRP')]\n",
            "2 - Long Sentence Verb+Noun: [('she', 'PRP'), ('wrote', 'VBD'), ('it', 'PRP'), ('to', 'TO'), ('him', 'PRP')]\n",
            "Remove 'To' and Swap PRP: [('she', 'PRP'), ('him', 'PRP'), ('it', 'PRP'), ('wrote', 'VBD')]\n",
            "Remove Pronouns Before Verbs: [('she', 'PRP'), ('him', 'PRP'), ('it', 'PRP'), ('wrote', 'VBD')]\n",
            "Pronoun+Verb+Noun: [('she', 'PRP'), ('him', 'PRP'), ('it', 'PRP'), ('wrote', 'VBD')]\n",
            "6 - Adjectives after Nouns: [('she', 'PRP'), ('him', 'PRP'), ('it', 'PRP'), ('wrote', 'VBD')]\n",
            "7 - Add 'the': [('she', 'PRP'), ('him', 'PRP'), ('it', 'PRP'), ('wrote', 'VBD')]\n",
            "8 - Attributive Nouns: [('she', 'PRP'), ('him', 'PRP'), ('it', 'PRP'), ('wrote', 'VBD')]\n",
            "9 - Move not after verbs: [('she', 'PRP'), ('him', 'PRP'), ('it', 'PRP'), ('wrote', 'VBD')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('she', 'PRP'), ('him', 'PRP'), ('it', 'PRP'), ('wrote', 'VBD')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDJ0b90IagIm",
        "outputId": "18e660cf-db84-43af-de0a-e3cab25061d2"
      },
      "source": [
        "print(nltk.pos_tag(nltk.word_tokenize((\"the too of\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 'DT'), ('too', 'RB'), ('of', 'IN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVqny7dyQMyX"
      },
      "source": [
        "## Spanish --> English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ll1gteCt_KZ"
      },
      "source": [
        "def toEnglishStyle(taggedWords):\n",
        "  #Rule 1: Noun + Adjective --> Adjective + Noun\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1: #for every tuple (word) in Array\n",
        "    if taggedWords[idx][1][0] == \"n\": #if current word is an noun\n",
        "      if taggedWords[idx+1][1][:2] == \"aq\": #if the proceding word is a adjective\n",
        "        taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx]\n",
        "    idx += 1\n",
        "  print(\"Adjectives before Nouns:\", taggedWords)\n",
        "\n",
        "  #Rule 2: attributive nouns (2nd noun = improper) and posessive nouns (2nd noun = proper)\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 2: #for every tuple (word) in Array\n",
        "    if taggedWords[idx][1][0] == \"n\": #if current word is an improper noun\n",
        "      if taggedWords[idx+1][0] == \"de\": #if the proceding word is \"de\"\n",
        "        if taggedWords[idx+2][1][:2] == \"nc\": #if the next word is an improper noun -- ATTRIBUTIVE NOUN\n",
        "          taggedWords[idx], taggedWords[idx+2] = taggedWords[idx+2], taggedWords[idx] #swap 1st and 2nd nouns\n",
        "          del taggedWords[idx+1] #delete \"de\"\n",
        "        elif taggedWords[idx+2][1][:2] == \"np\": #if 2nd noun = proper --> POSSESSIVE NOUN\n",
        "          taggedWords[idx], taggedWords[idx+2] = taggedWords[idx+2], taggedWords[idx] #swap nouns\n",
        "          taggedWords[idx+1:idx+2] = [(\"'de\", \"POS\")]\n",
        "    idx += 1\n",
        "  print(\"Attributive/Possesive Nouns:\", taggedWords)\n",
        "\n",
        "  #Rule 3: add pronouns to front of object-less verbs\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords): #for every tuple (word) in Array\n",
        "    if taggedWords[idx][1][:2] == \"vm\":\n",
        "      if (idx-1 < 0) | (taggedWords[idx-1][1][0] != \"n\"):\n",
        "        if taggedWords[idx][1][4] == \"1\": #1st person\n",
        "          if taggedWords[idx][1][5] == \"s\": #singular\n",
        "            taggedWords[idx:idx] = [(\"yo\", \"PRP\")]\n",
        "          else: #plural\n",
        "            taggedWords[idx:idx] = [(\"nosotros\", \"PRP\")]\n",
        "        elif taggedWords[idx][1][4] == \"3\": #3rd person\n",
        "          rand = (random.randint(0,9) < 5) \n",
        "          fAdjective = (taggedWords[idx-1][1][3] == \"f\")\n",
        "          mAdjective = (taggedWords[idx-1][1][3] == \"m\")\n",
        "          if taggedWords[idx][1][5] == \"s\": #singular\n",
        "            if (fAdjective == True) | (mAdjective == False & rand == True): #if adjective, use to determine gender of object. If not, choose randomly\n",
        "              taggedWords[idx:idx] = [(\"ella\", \"PRP\")]\n",
        "            else:\n",
        "              taggedWords[idx:idx] = [(\"él\", \"PRP\")]\n",
        "          else:\n",
        "            if (fAdjective == True) | (mAdjective == False & rand == True):\n",
        "              taggedWords[idx:idx] = [(\"ellas\", \"PRP\")]\n",
        "            else:\n",
        "              taggedWords[idx:idx] = [(\"ellos\", \"PRP\")]\n",
        "        else: #2nd person\n",
        "          if taggedWords[idx][1][5] == \"s\": #singular\n",
        "            taggedWords[idx:idx] = [(\"tu\", \"PRP\")]\n",
        "          else:\n",
        "            taggedWords[idx:idx] = [(\"vosotros\", \"PRP\")]  \n",
        "      \n",
        "        idx += 1\n",
        "    \n",
        "    idx += 1\n",
        "\n",
        "    \n",
        "  #omit la from beginning of plural direct objects\n",
        "\n",
        "  return taggedWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x59qB5jawHs"
      },
      "source": [
        "def changeSpanTags(taggedArray): #translating spaghetti POS codes to NLTK codes to potentially make syntax trees\n",
        "  idx = 0\n",
        "  while idx < len(taggedArray):\n",
        "    tag = \"\"\n",
        "\n",
        "    #Nouns#\n",
        "    if taggedArray[idx][1][0] == \"n\":\n",
        "      tag += \"NN\"\n",
        "      if taggedArray[idx][1][1] == \"p\": #proper\n",
        "        tag += \"P\"\n",
        "      if taggedArray[idx][1][4] == \"p\": #plural\n",
        "        tag += \"S\"\n",
        "\n",
        "    #Adjectives#\n",
        "    elif taggedArray[idx][1][:2] == \"aq\":\n",
        "      tag += \"JJ\"\n",
        "    \n",
        "    #Determiner#\n",
        "    elif (taggedArray[idx][1][:2] == \"da\") | (taggedArray[idx][1][:2] == \"dd\"):\n",
        "      tag += \"DT\"\n",
        "      \n",
        "    #Predeterminer#\n",
        "    elif (taggedArray[idx][1][:2] == \"di\") | (taggedArray[idx][1][:2] == \"dn\"):\n",
        "      tag += \"DT\"\n",
        "\n",
        "    #Prounouns#\n",
        "    elif taggedArray[idx][1][0] == \"p\": #quien and quienes are weird??\n",
        "      tag += \"PRP\"\n",
        "\n",
        "    elif taggedArray[idx][1][:2] == \"dp\":\n",
        "      tag += \"PRP$\"\n",
        "\n",
        "    #Adverbs#\n",
        "    elif taggedArray[idx][1][:2] == \"rg\":\n",
        "      tag += \"RB\"\n",
        "\n",
        "    #Verbs#\n",
        "    elif taggedArray[idx][1][:2] == \"vm\":\n",
        "      tag += \"VB\"\n",
        "      if taggedArray[idx][1][2] == \"n\": #infinitive\n",
        "        tag\n",
        "      elif taggedArray[idx][1][2] == \"g\": #gerand/present participle\n",
        "        tag += \"G\"\n",
        "      elif taggedArray[idx][1][2] == \"p\":\n",
        "        tag += \"N\"\n",
        "      elif taggedArray[idx][1][2] == \"i\": #conjugated\n",
        "        if taggedArray[idx][1][3] == \"p\": #present tense\n",
        "          if taggedArray[idx][1][4:6] == \"3s\":\n",
        "            tag += \"Z\"\n",
        "          else:\n",
        "            tag += \"P\"\n",
        "        elif (taggedArray[idx][1][3] == \"s\") | (taggedArray[idx][1][3] == \"i\"): #preterite vs. imperfect --> past\n",
        "          tag += \"D\"\n",
        "\n",
        "    #Coordinating Conjunctions#\n",
        "    elif taggedArray[idx][1][:2] == \"cc\":\n",
        "      tag += \"CC\"\n",
        "\n",
        "    #Prepositions#\n",
        "    elif (taggedArray[idx][1][:3] == \"sps\") | (taggedArray[idx][1][:2] == \"cs\"):\n",
        "      tag += \"IN\"\n",
        "\n",
        "    else:\n",
        "      tag += taggedArray[idx][1]\n",
        "  \n",
        "    taggedArray[idx] = (taggedArray[idx][0], tag)\n",
        "    idx += 1\n",
        "\n",
        "  return taggedArray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQ2NE-KQmvj"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axYtC-o7DZ7G"
      },
      "source": [
        "**Notes**\n",
        "* Spaghetti tagger doesn't know people names\n",
        "* Its grammar is also wrong surrounding adjectives... doesn't perceive blancos as an adjective (sees it as noun) but percieves blanco as adjective, even when paired with perros\n",
        "* nor does it keep track of verbs conjugated in first person?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOOtV-htCnID",
        "outputId": "696ebedf-ce0f-4252-e527-acb74cc99b5e"
      },
      "source": [
        "tokenized = nltk.word_tokenize(\"el perro blanco corrió\")\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['el', 'perro', 'blanco', 'corrió']\n",
            "[('el', 'da0ms0'), ('perro', 'ncms000'), ('blanco', 'aq0ms0'), ('corrió', 'vmis3s0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFjLsmWmDCBe",
        "outputId": "6be2da8c-7bd7-450c-8860-62bf2167c70f"
      },
      "source": [
        "tokenized = nltk.word_tokenize(\"el perro azul corre\")\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['el', 'perro', 'azul', 'corre']\n",
            "[('el', 'da0ms0'), ('perro', 'ncms000'), ('azul', 'aq0cs0'), ('corre', 'vmip3s0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbpEhCZvC4qf",
        "outputId": "6a48439d-41a9-46d6-806d-46403e67d439"
      },
      "source": [
        "tokenized = nltk.word_tokenize(\"los perros feo corren\")\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['los', 'perros', 'feo', 'corren']\n",
            "[('los', 'da0mp0'), ('perros', 'ncmp000'), ('feo', 'aq0ms0'), ('corren', 'vmip3p0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wxrs8CkYEWbb",
        "outputId": "53d2b75c-ef6a-4fc3-fe71-fb0d97569379"
      },
      "source": [
        "tokenized = nltk.word_tokenize(\"una chica bonita corren\")\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['una', 'chica', 'bonita', 'corren']\n",
            "[('una', 'di0fs0'), ('chica', 'ncfs000'), ('bonita', 'aq0fs0'), ('corren', 'vmip3p0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi5V8qMpDPPk",
        "outputId": "4f357435-5668-4c76-a4c0-013b1cb6e532"
      },
      "source": [
        "tokenized = nltk.word_tokenize(\"una chica bonita corre\")\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['una', 'chica', 'bonita', 'corre']\n",
            "[('una', 'di0fs0'), ('chica', 'ncfs000'), ('bonita', 'aq0fs0'), ('corre', 'vmip3s0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM2qSIHEIOid",
        "outputId": "32de1437-041e-44fb-c907-400663a4b81e"
      },
      "source": [
        "spanglishMachine(\"una chica bonita corre\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjectives before Nouns: [('una', 'di0fs0'), ('bonita', 'aq0fs0'), ('chica', 'ncfs000'), ('corre', 'vmip3s0')]\n",
            "Attributive/Possesive Nouns: [('una', 'di0fs0'), ('bonita', 'aq0fs0'), ('chica', 'ncfs000'), ('corre', 'vmip3s0')]\n",
            "after: una bonita chica corre\n",
            "after with tags: [('una', 'DT'), ('bonita', 'JJ'), ('chica', 'NN'), ('corre', 'VBZ')]\n",
            "before: una chica bonita corre\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXyqCICjHPR1",
        "outputId": "07fd2893-80f3-4bee-803a-8c350bffa2f4"
      },
      "source": [
        "spanglishMachine(\"café de Argentina\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjectives before Nouns: [('café', 'ncms000'), ('de', 'sps00'), ('Argentina', 'np0000l')]\n",
            "Attributive/Possesive Nouns: [('Argentina', 'np0000l'), (\"'de\", 'POS'), ('café', 'ncms000')]\n",
            "after: Argentina 'de café\n",
            "after with tags: [('Argentina', 'NNP'), (\"'de\", 'POS'), ('café', 'NN')]\n",
            "before: café de Argentina\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTwUFZWH2Ki4",
        "outputId": "e4c5d9f8-205e-41da-c78d-807a6fdafdeb"
      },
      "source": [
        "spanglishMachine(\"taza de café\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjectives before Nouns: [('taza', 'ncfs000'), ('de', 'sps00'), ('café', 'ncms000')]\n",
            "Attributive/Possesive Nouns: [('café', 'ncms000'), ('taza', 'ncfs000')]\n",
            "after: café taza\n",
            "after with tags: [('café', 'NN'), ('taza', 'NN')]\n",
            "before: taza de café\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o8IwxHwrUpm",
        "outputId": "fdebafb6-c079-4b65-aafd-5a531fa5ff23"
      },
      "source": [
        "spanglishMachine(\"corre\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjectives before Nouns: [('corre', 'vmip3s0')]\n",
            "Attributive/Possesive Nouns: [('corre', 'vmip3s0')]\n",
            "after: él corre\n",
            "after with tags: [('él', 'PRP'), ('corre', 'VBZ')]\n",
            "before: corre\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K_RFVdjEsS6"
      },
      "source": [
        "**Parsing POS Code Meaning**\n",
        "\n",
        "<u>Nouns</u>\n",
        "\n",
        "[0:2]\n",
        "* nc = regular noun\n",
        "* np = proper noun\n",
        "\n",
        "[3]\n",
        "* f = feminine\n",
        "* m = masculine\n",
        "\n",
        "[4]\n",
        "* s = singular\n",
        "* p = plural\n",
        "\n",
        "<u>Adjectives</u>\n",
        "\n",
        "[0:2] = aq\n",
        "\n",
        "[3]\n",
        "* f = feminine\n",
        "* m = masculine\n",
        "* c = gender nuetral\n",
        "\n",
        "[4]\n",
        "* s = singular\n",
        "* theoretically, p = plural, but spaghetti tagger doesn't seem to have many plural adjectives logged\n",
        "\n",
        "<u>Pronouns</u>\n",
        "\n",
        "[:2] = pp\n",
        "\n",
        "[2] = # = person (1st vs 3rd)\n",
        "\n",
        "[3]\n",
        "* m = masculine\n",
        "* f = feminine\n",
        "\n",
        "[4]\n",
        "* s = singular\n",
        "* p = plural\n",
        "\n",
        "<u>Possessive Pronounns</u>\n",
        "('nuestra', 'dp1fsp')\n",
        "\n",
        "[:2] = dp\n",
        "\n",
        "[2] = 1-3 person\n",
        "\n",
        "[3] = masc or fem\n",
        "\n",
        "[5] = fem or masc\n",
        "\n",
        "<u>Determiners</u>\n",
        "\n",
        "[0] = d\n",
        "\n",
        "[1]\n",
        "* a = el, la, los, las\n",
        "* i = un, una, unos, unas\n",
        "* d = esos, esta\n",
        "\n",
        "[2]\n",
        "* 0 = standard\n",
        "* 3 = possessive\n",
        "\n",
        "[3]\n",
        "* f = feminine\n",
        "* m = masculine\n",
        "\n",
        "[4]\n",
        "* s = singular\n",
        "* f = feminine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT5CjHjPQjR5"
      },
      "source": [
        "## Research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRiyEVEOMikU"
      },
      "source": [
        "[nltk Acronym Meanings](https://www.guru99.com/pos-tagging-chunking-nltk.html#:~:text=POS%20Tagging%20(Parts%20of%20Speech,is%20also%20called%20grammatical%20tagging.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO9DQu_L_EvG"
      },
      "source": [
        "[Grammar Diff Btwn Spanish and English](https://www.thoughtco.com/grammatical-differences-between-spanish-and-english-4119326#:~:text=Word%20order%20is%20less%20fixed,subjunctive%20mood%20than%20English%20does.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA82S6sS5K_F"
      },
      "source": [
        "***Sometimes Pronouns Come Before Verbs and Nouns After***\n",
        "\n",
        "Lo escribió Cervantes.\n",
        "*   Megan wrote it (the book) --> It (the book) wrote Cervantes\n",
        "*   NNP VBD PRP --> PRP VBD NNP\n",
        "\n",
        "\"No recuerdo el momento en que salió Pablo\"\n",
        "*   I don't remember the moment in which Pablo left --> I don't remember the moment in which left Pablo\n",
        "*   (PRP VBP RB VB DT NN IN WDT) NNP VBD --> (same) VBD NNP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCp1Ijm678FV"
      },
      "source": [
        "***Spanish Uses Double Negatives***\n",
        "\n",
        "* Apenas come. (She barely eats.)\n",
        "* Apenas come nada. (She barely eats anything --> she barely eats nothing)\n",
        "* No tengo ninguno. (I don't have any --> I don't have none)\n",
        "* Nadie sabe eso. (Nobody knows that.)\n",
        "* Jamás fumo. (I never smoke.)\n",
        "* Tampoco comió. (She didn't eat either.)\n",
        "* Tampoco comió nada. (She didn't eat anything either --> She didn't eat nothing either)\n",
        "* No habló. (He didn't speak.)\n",
        "* No dijo nada. (He said nothing --> He didn't say nothing)\n",
        "* No le dijo nada a nadie. (He didn't say anything to anybody --> He didn't say nothing to nobody)\n",
        "* No compro ninguno. (I'm not buying any --> I'm not buying none)\n",
        "* Nunca le compra nada a nadie. (She never buys anything for anybody --> She never buy nothing for nobody)\n",
        "* No come ni siquiera pan. (He doesn't even eat bread --> He doesnt --honestly I don't even know how to translate this literally)\n",
        "* Ni siquiera come pan. (He doesn't even eat bread.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cNVR48c9tA0"
      },
      "source": [
        "***Can Omit Pronouns if Verb is Conjugated***\n",
        "\n",
        "* No compro ninguno. (I'm not buying any --> I'm not buying none)\n",
        "* Nunca le compra nada a nadie. (She never buys anything for anybody --> She never buy nothing for nobody)\n",
        "* No come ni siquiera pan. (He doesn't even eat bread --> He doesnt --honestly I don't even know how to translate this literally)\n",
        "* Ni siquiera come pan. (He doesn't even eat bread.)\n",
        "\n",
        "*Especially* Don't need \"I\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AASQ8qUr-lRI"
      },
      "source": [
        "***Attributive Nouns***\n",
        "Not coffee cup --> taza para cafe --> cup of coffee\n",
        "\n",
        "if two nouns next to each other --> switch nouns and add \"of\" (para/de) between\n",
        "\n",
        "* In some cases, this is accomplished by Spanish having adjectival forms that don't exist in English. For example, informático can be the equivalent of \"computer\" as an adjective, so a computer table is a mesa informática."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYO7dHT41p_s"
      },
      "source": [
        "***Tacking Pronouns to End of Verbs When \"To\"***\n",
        " * darmelo = give it to me --> give me it\n",
        " * escribamostelo = let's write it to you --> let's write you it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4yH_HBz0BZf"
      },
      "source": [
        "## **English Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJG4occ7x8Tb",
        "outputId": "73e3bec1-fccd-4fa2-c8ba-a5fd3ca4fa68"
      },
      "source": [
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-kpFQz7J6uA"
      },
      "source": [
        "## **Spanish Data?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaDbFAtkOmKZ"
      },
      "source": [
        "# Read the corpus into a list, \n",
        "# each entry in the list is one sentence.\n",
        "cess_sents = cess.tagged_sents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBtEmt7FPZ8i"
      },
      "source": [
        "# Train the unigram tagger\n",
        "uni_tag = ut(cess_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEGYFga6P4No"
      },
      "source": [
        "spanishSentence = \"las chicas bonitas corrion\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_tyHxvV1zBY",
        "outputId": "e99c450c-8dc5-4fed-da77-ae205df1b5d9"
      },
      "source": [
        "#tokenized = spanishSentence.split()\n",
        "tokenized = nltk.word_tokenize(spanishSentence)\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)\n",
        "tagged2 = mytagger_uni.tag(tokenized)\n",
        "print(tagged2)\n",
        "tagged4 = mytagger_bi.tag(tokenized)\n",
        "print(tagged4)\n",
        "print(tagged4[0])\n",
        "print(tagged4[0][0]) #How to parce through intervals of the tuple in the array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['las', 'chicas', 'bonitas', 'corrion']\n",
            "[('las', 'da0fp0'), ('chicas', 'ncfp000'), ('bonitas', None), ('corrion', None)]\n",
            "[('las', 'da0fp0'), ('chicas', 'ncfp000'), ('bonitas', None), ('corrion', None)]\n",
            "[('las', 'da0fp0'), ('chicas', 'ncfp000'), ('bonitas', None), ('corrion', None)]\n",
            "('las', 'da0fp0')\n",
            "las\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7iAKLsWdED2"
      },
      "source": [
        "## **Syntax** **trees**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFvVZ0c9k2mE"
      },
      "source": [
        "import os\n",
        "import matplotlib as mpl\n",
        "if os.environ.get('DISPLAY','') == '':\n",
        "    print('no display found. Using non-interactive Agg backend')\n",
        "    mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzqAOhi6lQug",
        "outputId": "05c289e1-bac0-4d6e-ee76-b80c9f91fdc1"
      },
      "source": [
        "### CREATE VIRTUAL DISPLAY ###\n",
        "!apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
            "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
            "  404  Not Found [IP: 91.189.88.142 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_1.19.6-1ubuntu4.8_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_PeZhw_i_Mj",
        "outputId": "ef68648f-5bcd-4557-fc6b-81af0754775d"
      },
      "source": [
        "# EXTRA STUFF TO DISPLAY NLTK SYNTAX TREES #  \n",
        "%matplotlib inline\n",
        "### INSTALL GHOSTSCRIPT ) ###\n",
        "!apt install ghostscript python3-tk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ghostscript is already the newest version (9.26~dfsg+0-0ubuntu0.18.04.14).\n",
            "python3-tk is already the newest version (3.6.9-1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "lP1mbYPsfyfG",
        "outputId": "3d6d68ee-cdbd-41b2-d3ff-8bf4e5a74989"
      },
      "source": [
        "chunked_sentence = '(S (NP this tree) (VP (V is) (AdjP pretty)))'\n",
        "\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display\n",
        "tree = Tree.fromstring(str(chunked_sentence))\n",
        "display(tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: couldn't connect to display \":1.0\""
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', ['this', 'tree']), Tree('VP', [Tree('V', ['is']), Tree('AdjP', ['pretty'])])])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g-Y9HWMycIR",
        "outputId": "f9aa8886-5011-4ef6-b1ad-6520e070ae67"
      },
      "source": [
        "Sentence = \"The white dog ran\"\n",
        "tokenized = nltk.word_tokenize(Sentence)\n",
        "print(tokenized)\n",
        "tagged = nltk.pos_tag(tokenized)\n",
        "print(tagged)\n",
        "chunkGram = r\"\"\"Chunk: {<DT\\w?>*<JJ\\w?>}\"\"\" #creating a chunk\n",
        "chunkParser = nltk.RegexpParser(chunkGram) #parsing the chunk\n",
        "#chunked = chunkParser.parse(tagged)\n",
        "for tree in chunkParser.parse(tagged): #creates a syntax tree for the parsed chunk\n",
        "    print(tree)\n",
        "    #tree.draw()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'white', 'dog', 'ran']\n",
            "[('The', 'DT'), ('white', 'JJ'), ('dog', 'NN'), ('ran', 'VBD')]\n",
            "(Chunk The/DT white/JJ)\n",
            "('dog', 'NN')\n",
            "('ran', 'VBD')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-95_ks94yZr",
        "outputId": "27a39db2-3585-4507-9c30-7db407cfbfee"
      },
      "source": [
        "Sentence = \"white the ran dog.\"\n",
        "tokenized = nltk.word_tokenize(Sentence)\n",
        "print(tokenized)\n",
        "tagged = nltk.pos_tag(tokenized)\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['white', 'the', 'ran', 'dog', '.']\n",
            "[('white', 'JJ'), ('the', 'DT'), ('ran', 'NN'), ('dog', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0AR8EKfCHbP"
      },
      "source": [
        "## **Sanskrit Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1IRb82_D-Cd"
      },
      "source": [
        "Mounting Drive and importing text files of Sanskrit texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvEDZkqnEK-R",
        "outputId": "cebce4f4-ec09-4855-8b7d-315cfff243fc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqsUhQPYCObp"
      },
      "source": [
        "gItA = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/bhagavadgItA.txt\",\"r\")\n",
        "meghadhUta = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/meghadUta.txt\",\"r\")\n",
        "rAmAyaNa = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/rAmAyaNa.txt\", \"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "snXpa2v0F_9D",
        "outputId": "2b15ed1b-3c47-4e86-f73d-67b8d43ec2f4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-b5768a72344c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pos/hmm/make -f makefile-osx\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv8b6SgbHmJa"
      },
      "source": [
        "comment to delete later: trying to figure out how to use repository here : https://github.com/ad2476/pos-research which has a sanskrit tagger. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcPj9dYhsxH3"
      },
      "source": [
        "## PYCHARM CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThEBL-XEs0Wv"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy\n",
        "import IPython\n",
        "from IPython import display\n",
        "\n",
        "# making sure ghostscript works correctly so we can correctly convert .ps to .png for syntax tree\n",
        "os.environ[\"PATH\"] += \"Library/usr/local/bin\"\n",
        "os.environ[\"PATH\"] = \"/usr/local/bin:\" + os.environ[\"PATH\"]\n",
        "os.environ[\"PATH\"] = \"Library/usr/local/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "\n",
        "#import necessary toolboxes\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "import pygame\n",
        "from pygame.locals import *\n",
        "\n",
        "#initialize user input of english sentence\n",
        "userSentence = input(\"Please enter a sentence under 15 words:\\n\")\n",
        "\n",
        "# create syntax tree that displays chunks\n",
        "def syntaxTreeGenerator(array):\n",
        "    for item in array:\n",
        "        tokenized = nltk.word_tokenize(item)\n",
        "        print(\"tokenized = \", tokenized)\n",
        "        tagged = nltk.pos_tag(tokenized)\n",
        "        print(\"tagged = \", tagged)\n",
        "\n",
        "        chunkGram = r\"\"\"Chunk: {<DT\\w?>*<JJ\\w?>}\"\"\" #creating a chunk\n",
        "        chunkParser = nltk.RegexpParser(chunkGram) #parsing the chunk\n",
        "\n",
        "        chunked = chunkParser.parse(tagged)\n",
        "        print(chunked)\n",
        "        #chunked.draw()\n",
        "\n",
        "        ### Save syntax tree as file ###\n",
        "        from nltk.draw.tree import TreeView\n",
        "        TreeView(chunked)._cframe.print_to_file('/Users/catherine/PycharmProjects/CLPS0950FinalProject/tree.ps')\n",
        "\n",
        "        ### Convert .ps file to .png\n",
        "        from PIL import Image\n",
        "        psimage = Image.open(r'/Users/catherine/PycharmProjects/CLPS0950FinalProject/tree.ps')\n",
        "        psimage.save(r'/Users/catherine/PycharmProjects/CLPS0950FinalProject/pygame_tree.png')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "syntaxTreeGenerator([userSentence])\n",
        "\n",
        "#initialize display window for pygame\n",
        "pygame.init()\n",
        "width, height = 1400, 720\n",
        "screen = pygame.display.set_mode((width, height))\n",
        "\n",
        "#define colors\n",
        "black = (0,0,0)\n",
        "white = (255, 255, 255)\n",
        "\n",
        "# pygame window name\n",
        "pygame.display.set_caption('Spanglish Generator')\n",
        "\n",
        "#font\n",
        "font = pygame.font.Font('freesansbold.ttf', 32)\n",
        "font2 = pygame.font.Font('freesansbold.ttf', 20)\n",
        "\n",
        "# text surface object\n",
        "titleText = font.render('Spanglish Generator', True, black, white)\n",
        "textRect = titleText.get_rect()\n",
        "\n",
        "syntaxTreeText = font2.render('Syntax Tree', True, black, white)\n",
        "rectSyntaxTreeText = syntaxTreeText.get_rect()\n",
        "#rectSyntaxTreeText.w = width//2\n",
        "\n",
        "\n",
        "# load images and center if need be\n",
        "background = pygame.image.load(\"/Users/catherine/Desktop/project_background.png\")\n",
        "syntaxTree = pygame.image.load(\"/Users/catherine/PycharmProjects/CLPS0950FinalProject/pygame_tree.png\")\n",
        "rectSyntaxTree = syntaxTree.get_rect()\n",
        "rectSyntaxTree.center = (width//2, 400)\n",
        "screen_rect = screen.get_rect()\n",
        "\n",
        "#rectSyntaxTree.center = (width//2, height//2)\n",
        "\n",
        "# loop through\n",
        "while True:\n",
        "    #allows user to exit screen\n",
        "    for event in pygame.event.get():\n",
        "        # check if event is the X button\n",
        "        if event.type == pygame.QUIT:\n",
        "            # if yes then quite game\n",
        "            pygame.quit()\n",
        "            sys.exit()\n",
        "    #clear the screen before putting things in again\n",
        "    screen.fill(white)\n",
        "\n",
        "    #draw screen elements\n",
        "    for x in range(width // background.get_width() + int(1)):\n",
        "        for y in range(height // background.get_height() + int(1)):\n",
        "            screen.blit(background, (x * 100, y * 100))\n",
        "    screen.blit(titleText, titleText.get_rect(midtop=screen_rect.midtop)) #says syntax generator at top of screen\n",
        "    screen.blit(syntaxTreeText, (640, 260))\n",
        "    #screen.blit(syntaxTreeText, syntaxTreeText.get_rect(w=rectSyntaxTreeText.w))\n",
        "    screen.blit(syntaxTree, syntaxTree.get_rect(center=rectSyntaxTree.center))\n",
        "\n",
        "\n",
        "    #screen.blit(print(syntaxTreeGenerator([userSentence])), (50, 50)) this doesn't work; shuts down\n",
        "\n",
        "\n",
        "    #Update the screen\n",
        "    pygame.display.flip()\n",
        "    # loop through events\n",
        "   # for event in pygame.event.get():\n",
        "        #check if event is the X button\n",
        "      #  if event.type==pygame.QUIT:\n",
        "            # if yes then quite game\n",
        "        #    pygame.quit()\n",
        "          #  sys.exit()\n",
        "\n",
        "\n",
        "            #quit()\n",
        "        #pygame.display.update()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#initialize user input of english sentence\n",
        "userSentence = input(\"Please enter a sentence:\\n\")\n",
        "#userSentenceList = list(userSentence.split(\" \"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create syntax tree that displays chunks\n",
        "def syntaxTreeGenerator(array):\n",
        "    for item in array:\n",
        "        tokenized = nltk.word_tokenize(item)\n",
        "        print(\"tokenized = \", tokenized)\n",
        "        tagged = nltk.pos_tag(tokenized)\n",
        "        print(\"tagged = \", tagged)\n",
        "\n",
        "        chunkGram = r\"\"\"Chunk: {<DT\\w?>*<JJ\\w?>}\"\"\" #creating a chunk\n",
        "        chunkParser = nltk.RegexpParser(chunkGram) #parsing the chunk\n",
        "\n",
        "        chunked = chunkParser.parse(tagged)\n",
        "        print(chunked)\n",
        "        chunked.draw()\n",
        "\n",
        "#calling the function\n",
        "exampleArray = [\"The white dog ran\"]\n",
        "syntaxTreeGenerator([userSentence])\n",
        "#syntaxTreeGenerator(exampleArray)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#for item in exampleArray:\n",
        "  #  tokenized = nltk.word_tokenize(item)\n",
        "  #  print(tokenized)\n",
        "   # tagged = nltk.pos_tag(tokenized)\n",
        "   # print(tagged)\n",
        "\n",
        "   # chunkGram = r\"\"\"Chunk: {<DT\\w?>*<JJ\\w?>}\"\"\" #creating a chunk\n",
        "   # chunkParser = nltk.RegexpParser(chunkGram) #parsing the chunk\n",
        "\n",
        "   # chunked = chunkParser.parse(tagged)\n",
        "   ## chunked.draw()\n",
        "\n",
        "#for tree in chunkParser.parse(tagged): #creates a syntax tree for the parsed chunk\n",
        "   # print(tree)\n",
        "   # chunked.draw()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}