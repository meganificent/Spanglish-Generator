{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project",
      "provenance": [],
      "collapsed_sections": [
        "W_G0P5DTJwH3"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meganificent/Language-Style-Transfer/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTGBelac_NaI"
      },
      "source": [
        "1.\tInsert English phrase\n",
        "a.\tTokenize English phrase --> get POS array of tuple\n",
        "2.\tTranslate English phrase to Spanish using Google Translate\n",
        "3.\tToken Spanish phrase\n",
        "4.\tSpaghetti tokenized Spanish phrase --> output array of tuples with Spanish word as the key and pos as the value\n",
        "5.\tGo through each key of POS English phrase and compare to keys of POS Spanish phrases --> move all English object intervals so that they are in the same positions as their translated Spanish word counterparts\n",
        "6.\tMerge rearranged array of objects so that their keys produce one potentially grammatically incoherent English sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl8B52Ad_WED"
      },
      "source": [
        "Special cases: \"a\" in spanish, \"to\" for location in English --> brainstorm placeholders for these kinds of things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12swv4FovvTs"
      },
      "source": [
        "import nltk\n",
        "import numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNrrF_WYDXEP"
      },
      "source": [
        "from __future__ import print_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOSsb-duk_Ot",
        "outputId": "259f7b0a-b284-429c-e87d-136ef34be180"
      },
      "source": [
        "import spaghetti as sp\n",
        "import nltk\n",
        "nltk.download('cess_esp')\n",
        "mytagger = sp.CESSTagger()\n",
        "mytagger_uni = mytagger.uni\n",
        "mytagger_bi = mytagger.bi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "*** First-time use of cess tagger ***\n",
            "Training tagger ...\n",
            "Tagger trained with cess using UnigramTagger and BigramTagger.\n",
            "Tagger trained with cess_nomwe using UnigramTagger and BigramTagger.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wXdg027CAaA"
      },
      "source": [
        "### **Google Translate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gupDVPEVCNXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5103766a-a8a2-49c0-8662-3c16b22d613e"
      },
      "source": [
        "!pip install googletrans==3.1.0a0\n",
        "import googletrans\n",
        "print(googletrans.LANGUAGES)\n",
        "from googletrans import Translator"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.4.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.22)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "{'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'he': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'or': 'odia', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'ug': 'uyghur', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMZv6MloFg0y"
      },
      "source": [
        "translator = Translator(service_urls=['translate.googleapis.com'])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms4UFFebztCW",
        "outputId": "4407baf7-539f-4441-e6cb-7238002dd629"
      },
      "source": [
        "result = translator.translate(\"el perro blanco corrió\", dest=\"en\")\n",
        "print(result.text)\n",
        "print(result.src)\n",
        "print(result.dest)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the white dog ran\n",
            "es\n",
            "en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4yH_HBz0BZf"
      },
      "source": [
        "### **English Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJG4occ7x8Tb",
        "outputId": "7e13b288-88a8-45c5-cd21-4c41f02559d2"
      },
      "source": [
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g-Y9HWMycIR",
        "outputId": "5e4562b4-3f9f-4e1c-965f-ccf72f3e8d99"
      },
      "source": [
        "Sentence = \"The white dog ran\"\n",
        "tokenized = nltk.word_tokenize(Sentence)\n",
        "print(tokenized)\n",
        "tagged = nltk.pos_tag(tokenized)\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'white', 'dog', 'ran']\n",
            "[('The', 'DT'), ('white', 'JJ'), ('dog', 'NN'), ('ran', 'VBD')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-95_ks94yZr",
        "outputId": "69f4be20-73f6-4b4c-adb2-5ec20d7c7d1c"
      },
      "source": [
        "Sentence = \"white the ran dog.\"\n",
        "tokenized = nltk.word_tokenize(Sentence)\n",
        "print(tokenized)\n",
        "tagged = nltk.pos_tag(tokenized)\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['white', 'the', 'ran', 'dog', '.']\n",
            "[('white', 'JJ'), ('the', 'DT'), ('ran', 'NN'), ('dog', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-kpFQz7J6uA"
      },
      "source": [
        "### **Spanish Data?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxr9qSXUSrAn",
        "outputId": "3f0fda82-d5fc-4030-ced0-3ddba92be74b"
      },
      "source": [
        "!git clone https://github.com/alvations/spaghetti-tagger.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'spaghetti-tagger' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6UhXZTIXcCE",
        "outputId": "40dc9b92-3d4f-4a92-ae71-c9865ab4b4e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/cont\n",
        "ent/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foieQg6LOart",
        "outputId": "5093cc11-6bbf-4cab-e8c2-b2736845af19"
      },
      "source": [
        "nltk.corpus.cess_esp.words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['El', 'grupo', 'estatal', 'Electricité_de_France', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tolSY-XOki1",
        "outputId": "9e544efc-2615-457a-b064-54455b4d4f54"
      },
      "source": [
        "nltk.corpus.cess_esp.sents()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdRSXGv6MLhA",
        "outputId": "c40cfac7-40e3-4b79-9399-d230b25c76eb"
      },
      "source": [
        "nltk.download(\"cess_esp\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRUUYWeU1MlA"
      },
      "source": [
        "from nltk.corpus import cess_esp as cess\n",
        "from nltk import UnigramTagger as ut\n",
        "from nltk import BigramTagger as bt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaDbFAtkOmKZ"
      },
      "source": [
        "# Read the corpus into a list, \n",
        "# each entry in the list is one sentence.\n",
        "cess_sents = cess.tagged_sents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBtEmt7FPZ8i"
      },
      "source": [
        "# Train the unigram tagger\n",
        "uni_tag = ut(cess_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEGYFga6P4No"
      },
      "source": [
        "spanishSentence = \"el perro blanco corrió\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_tyHxvV1zBY",
        "outputId": "aa60c5ba-7b3f-4e18-849f-7ef8d8ec5b18"
      },
      "source": [
        "tokenized = spanishSentence.split()\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)\n",
        "tagged2 = mytagger_uni.tag(tokenized)\n",
        "print(tagged2)\n",
        "tagged4 = mytagger_bi.tag(tokenized)\n",
        "print(tagged4)\n",
        "print(tagged4[0])\n",
        "print(tagged4[0][0]) #How to parce through intervals of the tuple in the array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['el', 'perro', 'blanco', 'corrió']\n",
            "[('el', 'da0ms0'), ('perro', 'ncms000'), ('blanco', 'aq0ms0'), ('corrió', 'vmis3s0')]\n",
            "[('el', 'da0ms0'), ('perro', 'ncms000'), ('blanco', 'aq0ms0'), ('corrió', 'vmis3s0')]\n",
            "[('el', 'da0ms0'), ('perro', 'ncms000'), ('blanco', 'aq0ms0'), ('corrió', 'vmis3s0')]\n",
            "('el', 'da0ms0')\n",
            "el\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEfuWamYEmjp",
        "outputId": "52e2e04c-abfb-4042-927e-4d81d28d320b"
      },
      "source": [
        "femSentence = \"la niña es fea\"\n",
        "tokenized1 = femSentence.split()\n",
        "print(tokenized1)\n",
        "tagged3 = sp.pos_tag(tokenized1)\n",
        "print(tagged3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['la', 'niña', 'es', 'fea']\n",
            "[('la', 'da0fs0'), ('niña', 'ncfs000'), ('es', 'vsip3s0'), ('fea', 'aq0fs0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUsKWxc9Pvs7",
        "outputId": "3597b30d-3846-44ee-ab13-3a9b2a95d7e0"
      },
      "source": [
        "# Split corpus into training and testing set.\n",
        "train = int(len(cess_sents)*90/100) # 90%\n",
        "\n",
        "# Train a bigram tagger with only training data.\n",
        "bi_tag = bt(cess_sents[:train], backoff=uni_tag)\n",
        "\n",
        "# Evaluates on testing data remaining 10%\n",
        "bi_tag.evaluate(cess_sents[train+1:])\n",
        "\n",
        "# Using the tagger.\n",
        "bi_tag.tag(Sentence.split(\" \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('el', 'da0ms0'),\n",
              " ('perro', 'ncms000'),\n",
              " ('blanco', 'aq0ms0'),\n",
              " ('corrió', 'vmis3s0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "G4lJo9d67yBw",
        "outputId": "e502f828-e55b-438a-e084-375dd60b8e0b"
      },
      "source": [
        "nltk.corpus.genesis.tagged_words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-673971ba013a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PlaintextCorpusReader' object has no attribute 'tagged_words'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0AR8EKfCHbP"
      },
      "source": [
        "# **Sanskrit Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1IRb82_D-Cd"
      },
      "source": [
        "Mounting Drive and importing text files of Sanskrit texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvEDZkqnEK-R",
        "outputId": "cebce4f4-ec09-4855-8b7d-315cfff243fc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqsUhQPYCObp"
      },
      "source": [
        "gItA = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/bhagavadgItA.txt\",\"r\")\n",
        "meghadhUta = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/meghadUta.txt\",\"r\")\n",
        "rAmAyaNa = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/rAmAyaNa.txt\", \"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "snXpa2v0F_9D",
        "outputId": "2b15ed1b-3c47-4e86-f73d-67b8d43ec2f4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-b5768a72344c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pos/hmm/make -f makefile-osx\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv8b6SgbHmJa"
      },
      "source": [
        "comment to delete later: trying to figure out how to use repository here : https://github.com/ad2476/pos-research which has a sanskrit tagger. "
      ]
    }
  ]
}