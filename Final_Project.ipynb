{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project",
      "provenance": [],
      "collapsed_sections": [
        "K0AR8EKfCHbP"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meganificent/Language-Style-Transfer/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTGBelac_NaI"
      },
      "source": [
        "1.\tInsert English phrase\n",
        "a.\tTokenize English phrase --> get POS array of tuple\n",
        "2.\tTranslate English phrase to Spanish using Google Translate\n",
        "3.\tToken Spanish phrase\n",
        "4.\tSpaghetti tokenized Spanish phrase --> output array of tuples with Spanish word as the key and pos as the value\n",
        "5.\tGo through each key of POS English phrase and compare to keys of POS Spanish phrases --> move all English object intervals so that they are in the same positions as their translated Spanish word counterparts\n",
        "6.\tMerge rearranged array of objects so that their keys produce one potentially grammatically incoherent English sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl8B52Ad_WED"
      },
      "source": [
        "Special cases: \"a\" in spanish, \"to\" for location in English --> brainstorm placeholders for these kinds of things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeyFvC5nt3D7",
        "outputId": "35905499-7675-4ebb-e648-1b7e9eca2a73"
      },
      "source": [
        "spanglishMachine(\"I have not had any index errors with my matrix bullshit yet\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: I have not had any index errors with my matrix bullshit yet\n",
            "after: have not had any errors of index with my bullshit of matrix yet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G944WiS3dM82"
      },
      "source": [
        "### Stuff to Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12swv4FovvTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd5921d-e1ac-4394-d05b-10ffb86e5a28"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "import numpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNrrF_WYDXEP"
      },
      "source": [
        "from __future__ import print_function"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOSsb-duk_Ot",
        "outputId": "df776b27-8ba2-478b-8881-0ff26c5521b9"
      },
      "source": [
        "import spaghetti as sp\n",
        "import nltk\n",
        "nltk.download('cess_esp')\n",
        "mytagger = sp.CESSTagger()\n",
        "mytagger_uni = mytagger.uni\n",
        "mytagger_bi = mytagger.bi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "*** First-time use of cess tagger ***\n",
            "Training tagger ...\n",
            "Tagger trained with cess using UnigramTagger and BigramTagger.\n",
            "Tagger trained with cess_nomwe using UnigramTagger and BigramTagger.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ippBa9j5chha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d25b920-7df8-4d6c-85ec-c19a436f2505"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "syns = wordnet.synsets(\"up\")\n",
        "print(syns[2].lemmas()[0].antonyms())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBDrN7PddoFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c43458-a618-4646-a327-186b54cc6194"
      },
      "source": [
        "\tsynonyms = []\n",
        "\tantonyms = []\n",
        "\n",
        "\tfor syn in wordnet.synsets(\"none\"):\n",
        "\t\tfor l in syn.lemmas():\n",
        "\t\t\tsynonyms.append(l.name())\n",
        "\t\t\tif l.antonyms():\n",
        "\t\t\t\t antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "\tprint(set(synonyms))\n",
        "\tprint(set(antonyms))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'none'}\n",
            "set()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gupDVPEVCNXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e14d3162-038a-4c56-a5af-794a917f8371"
      },
      "source": [
        "!pip install googletrans==3.1.0a0\n",
        "import googletrans\n",
        "print(googletrans.LANGUAGES)\n",
        "from googletrans import Translator"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 6.5MB/s \n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.9MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=c0be3eb2beb9b5e138e5c3b961b59bdfec73c1fa08a3bab459aa945e76d171a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hstspreload, h11, sniffio, hpack, hyperframe, h2, httpcore, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.4.0 sniffio-1.2.0\n",
            "{'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'he': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'or': 'odia', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'ug': 'uyghur', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMZv6MloFg0y"
      },
      "source": [
        "translator = Translator(service_urls=['translate.googleapis.com'])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wXdg027CAaA"
      },
      "source": [
        "### **Google Translate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms4UFFebztCW",
        "outputId": "701e7cf8-6b5e-4c8c-bbbb-ee6edee6c590"
      },
      "source": [
        "result = translator.translate(\"el perro blanco corrió\", dest=\"en\")\n",
        "print(result.text)\n",
        "print(result.src)\n",
        "print(result.dest)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the white dog ran\n",
            "es\n",
            "en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kaAWdpZBqzq"
      },
      "source": [
        "### Spanglish Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nZJ6VMCwta7",
        "outputId": "f3759a21-ab5a-48e9-d3e5-bdd96a2cbcdc"
      },
      "source": [
        "spanglishMachine(\"white coffee cup spilled him\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: white coffee cup spilled him\n",
            "after: him spilled the cup of coffee white\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnV9pAZlq1Mc",
        "outputId": "e84a1839-0904-4ea5-b989-711227a31120"
      },
      "source": [
        "spanglishMachine(\"Megan wrote it\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: Megan wrote it\n",
            "after: it wrote Megan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxtwzGXLwz9K",
        "outputId": "af277ef4-acf1-444e-83e3-4c003c82ecd3"
      },
      "source": [
        "spanglishMachine(\"I write but the hot sun stays down\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: I write but the hot sun stays down\n",
            "after: write but stays the sun hot down\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI0PEOKPUDmr",
        "outputId": "fffc9ff0-8485-4f95-f697-beff608ae5ba"
      },
      "source": [
        "spanglishMachine(\"I wrote while the blue wolf cried to the moon\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: I wrote while the blue wolf cried to the moon\n",
            "after: wrote while cried the wolf blue to the moon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XZXbCINXg6a",
        "outputId": "fe2716ef-5538-4438-bb82-7a43f3fe956e"
      },
      "source": [
        "spanglishMachine(\"I wrote while blue coffee cups cried to the moon\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: I wrote while blue coffee cups cried to the moon\n",
            "after: wrote while cried the cups of coffee blue to the moon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TdJQd4o_tw3",
        "outputId": "6810a78c-3b79-4b6d-aa8f-adab53855cba"
      },
      "source": [
        "spanglishMachine(\"blue sister's heart cried\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: blue sister 's heart cried\n",
            "after: the heart cried of the sister blue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie8Rxo-oBvmW",
        "outputId": "94e103f8-dc92-472f-d222-e4a58de94381"
      },
      "source": [
        "spanglishMachine(\"I wrote while Joanna's blue coffee cups cried to it\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: I wrote while Joanna 's blue coffee cups cried to it\n",
            "after: wrote while cried the cups of coffee blue of Joanna to it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ads14dBgYnn",
        "outputId": "63eafcde-4129-4fd5-9b8f-7e91d653baa0"
      },
      "source": [
        "spanglishMachine(\"she wrote it to him\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: she wrote it to him\n",
            "after: she him it wrote\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60JXLI0rRHRD"
      },
      "source": [
        "def spanglishMachine(string): \n",
        "  origTokenized = nltk.word_tokenize(string)\n",
        "\n",
        "  #determine language of inserted string and feed into correct translation function\n",
        "  \n",
        "  translated = translator.translate(string, dest = \"en\") #initially translate to english\n",
        "  if translated.src == \"en\": #if the original language was english\n",
        "    tagged = nltk.pos_tag(origTokenized) #tag POS using nltk\n",
        "    #print(\"Original:\", tagged)\n",
        "    \n",
        "    #old sentence for testing\n",
        "    originalString = \" \".join([word[0] for word in tagged])\n",
        "    #print(\"before:\", originalString)\n",
        "\n",
        "    translatedArray = toSpanishStyle(tagged) #transfer to Spanish\n",
        "    \n",
        "    #new sentence for testing\n",
        "    translatedString = \" \".join([word[0] for word in translatedArray])\n",
        "    print(\"before:\", originalString)\n",
        "    print(\"after:\", translatedString)\n",
        "\n",
        "  else: #if original language was spanish\n",
        "    tagged = sp.pos_tag(origTokenized) #tag POS using spaghetti\n",
        "    print(\"Original:\", tagged)\n",
        "    translated = toEnglishStyle(tagged) #transfer to English\n",
        "    #print(translated)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXAHHc3LoUsa",
        "outputId": "75d6a849-f5bf-4e5b-ae91-949d62d0e334"
      },
      "source": [
        "fi = [\"hi\", \"my\", \"tired\", \"mind\"]\n",
        "fi[3:3] = \"beautiful\", \"lost\"\n",
        "print(fi)\n",
        "fi[4:4] = fi[1:3]\n",
        "print(fi)\n",
        "del fi[1:3]\n",
        "print(fi)\n",
        "print(len(fi))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hi', 'my', 'tired', 'beautiful', 'lost', 'mind']\n",
            "['hi', 'my', 'tired', 'beautiful', 'my', 'tired', 'lost', 'mind']\n",
            "['hi', 'beautiful', 'my', 'tired', 'lost', 'mind']\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqcAIiut2OT"
      },
      "source": [
        "def toSpanishStyle(taggedWords):\n",
        "  #Rule 1: Remove P\n",
        "  #taggedWords = [word for word in taggedWords if word[0] != \"I\"]\n",
        "\n",
        "  #Eventually need to figure out how to treat posessive proper nouns --> should be paired with noun. Need to conver before Attributive Nouns step\n",
        "\n",
        "  #Rule 4: Possessive Nouns After What they Possess\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords)-1:\n",
        "    if taggedWords[idx][1] == \"POS\": #if current word is a possessive ending (\"'s\")\n",
        "      #print(idx)\n",
        "      #possessive + noun and all its qualifiers\n",
        "      posNounEnd = idx+1\n",
        "      #print(\"posNounEnd = \", taggedWords[posNounEnd])\n",
        "      posNounStart = getNounStart(posNounEnd, taggedWords)\n",
        "      #noun + all its qualifiers of the possessed noun\n",
        "      ownedNounStart = idx+1\n",
        "      #print(\"ownedNounStart = \", taggedWords[ownedNounStart])\n",
        "      ownedNounEnd = idx+2 #added 1 for the sake of indexing later\n",
        "      if idx+2 < len(taggedWords): #avoid runtime errors\n",
        "        if taggedWords[idx+2][1] == \"NN\":\n",
        "          ownedNounEnd = idx+3\n",
        "          if idx+3 < len(taggedWords): #avoid runtime errors\n",
        "            if taggedWords[idx+3][1] == \"NN\":\n",
        "              ownedNounEnd = idx+4\n",
        "      #move posNoun after ownedNoun\n",
        "      taggedWords[ownedNounEnd+1:ownedNounEnd+1] = nltk.pos_tag(nltk.word_tokenize(\"of\")) #insert \"of\" after posessed noun\n",
        "      taggedWords[ownedNounEnd+2:ownedNounEnd+2] = taggedWords[posNounStart:posNounEnd-1] #add posessive noun stuff (minus 's) after owned noun stuff's \"of\"\n",
        "      del taggedWords[posNounStart:posNounEnd] #remove original pos noun stuff\n",
        "\n",
        "      idx+=1 #so dont recount things\n",
        "    \n",
        "    idx +=1\n",
        "  #print(\"Posessive Nouns pt 1:\", taggedWords)\n",
        "\n",
        "  #Rule 3.5: Noun after Verb When It's a Verb applied to the second noun in the Sentence (because switch more common with long sentences)\n",
        "  idx = 0\n",
        "  numSubjects = 0\n",
        "  while idx < len(taggedWords)-1:\n",
        "    if (taggedWords[idx][1][:2] == \"NN\") | (taggedWords[idx][1] == \"PRP\"): #if current word is a noun\n",
        "      if taggedWords[idx+1][1][:2] == \"VB\":        \n",
        "        numSubjects += 1\n",
        "        if numSubjects == 2: #if currently on second noun with verb\n",
        "          nounEnd = idx+1 # idx-1 + 1 for the sake of indexing later\n",
        "          nounStart = getNounStart(nounEnd, taggedWords) #where noun's qualifier's start\n",
        "          \n",
        "         # if nounEnd + 2 <len(taggedWords):\n",
        "         #   if taggedWords[nounEnd+1][0] == \"to\": #if now has \"to\" after it\n",
        "         #     if taggedWords[nounEnd+2][1] == \"PRP\": #and 'to' has a pronoun after it\n",
        "         #       del taggedWords[nounEnd+1]\n",
        "         #       taggedWords[nounEnd+1] = (taggedWords[nounEnd+1][0], \"unmutable\")\n",
        "         #       taggedWords[nounEnd], taggedWords[nounEnd+1] = taggedWords[nounEnd+1], taggedWords[nounEnd] #swap VB and PRP\n",
        "          taggedWords[idx+2:idx+2] = taggedWords[nounStart:nounEnd] #add noun stuff after verb\n",
        "          del taggedWords[nounStart:nounEnd] #remove ealier noun elements\n",
        "    \n",
        "    idx += 1\n",
        "  #print(\"Long Sentence Verb+Noun:\", taggedWords)\n",
        "\n",
        "\n",
        "  #Rule ?: verb+ pronoun1 + to + pronoun2 --> pronoun 2 + pronoun 1 + verb\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 3:\n",
        "    if taggedWords[idx][1][:2] == \"VB\":\n",
        "      if taggedWords[idx+1][1] == \"PRP\":\n",
        "        if taggedWords[idx+2][0] == \"to\":\n",
        "          if taggedWords[idx+3][1] == \"PRP\":\n",
        "            taggedWords[idx+1], taggedWords[idx+3] = taggedWords[idx+3], taggedWords[idx+1]\n",
        "            del taggedWords[idx+2] #delete \"to\"\n",
        "            taggedWords[idx:idx] = taggedWords[idx+1:idx+3]\n",
        "            del taggedWords[idx+3:idx+5]\n",
        "            idx+=1\n",
        "    idx+=1\n",
        "  #print(\"Remove 'To' and Swap PRP:\", taggedWords)\n",
        "\n",
        "  #Rule 1: Remove Pronouns Directly Before Conjugated Verbs If not 2 or 3 pronoun group before verb\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1:\n",
        "    if taggedWords[idx][1] == \"PRP\":\n",
        "      if taggedWords[idx+1][1][:2] == \"VB\":\n",
        "        if idx-1 >= 0:\n",
        "          if taggedWords[idx-1][1] != \"PRP\":\n",
        "            del taggedWords[idx]\n",
        "        else:\n",
        "          del taggedWords[idx]\n",
        "    idx += 1\n",
        "  #print(\"Remove Pronouns Before Verbs:\", taggedWords)\n",
        "\n",
        "  #Rule 3: Pronoun + Verb + Noun\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1:\n",
        "    if taggedWords[idx][1][:2] == \"VB\": #if current word is a verb\n",
        "      #print(idx)\n",
        "      if taggedWords[idx+1][1] == \"PRP\": #if there's a pronoun directly after the verb\n",
        "        if idx-1 >= 0: #to avoid runtime errors\n",
        "          if taggedWords[idx-1][1][:2] == \"NN\": #if there's a noun right before the verb\n",
        "            nounEnd = idx # idx-1 + 1 for the sake of indexing later\n",
        "            nounStart = getNounStart(nounEnd, taggedWords) #where noun's qualifier's start\n",
        "            if idx+2 < len(taggedWords):\n",
        "              if taggedWords[idx+2] == \"PRP\": #if there's a pronoun verb directly after it\n",
        "                taggedWords[idx-1] = taggedWords[idx+1:idx+3]\n",
        "                del taggedWords[idx+1:idx+3] #more both pronouns before verb\n",
        "              else:\n",
        "                taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx] #swap pronoun and verb positions\n",
        "            else:\n",
        "              taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx] #swap pronoun and verb positions\n",
        "            taggedWords[idx+2:idx+2] = taggedWords[nounStart:nounEnd] #add noun stuff after where pronoun used to be\n",
        "            del taggedWords[nounStart:nounEnd] #remove ealier noun elements\n",
        "            idx +=1 #so I don't recount verb\n",
        "\n",
        "    idx +=1\n",
        "  #print(\"Pronoun+Verb+Noun:\", taggedWords)\n",
        "\n",
        "\n",
        "\n",
        "  #Rule 4: Adjectives After Nouns\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1: #for every tuple (word) in Array\n",
        "    if taggedWords[idx][1][:2] == \"JJ\": #if current word is an adjective (can tell general tag by first 2 letters of acronym)\n",
        "      if taggedWords[idx+1][1][:2] == \"NN\": #if the proceding word is a noun #how do I avoid an error if adjective is last word in list?\n",
        "        taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx]\n",
        "    idx += 1\n",
        "  #print(\"Adjectives after Nouns:\", taggedWords)\n",
        "\n",
        "\n",
        "  #Rule 5: \"The\" Before Every Non-Proper Noun\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords):\n",
        "    if (taggedWords[idx][1][:2] == \"NN\") & (taggedWords[idx][1][:3] != \"NNP\"): #if word is a noun and not a proper noun\n",
        "      if idx == 0:\n",
        "        taggedWords[idx:idx] = nltk.pos_tag(nltk.word_tokenize(\"the\"))\n",
        "        idx += 1 #so we don't recount nouns\n",
        "      else: #to avoid runtime errors\n",
        "        if (taggedWords[idx - 1][1] != \"DT\") & (taggedWords[idx - 1][1] != \"PRP$\") & (taggedWords[idx - 1][1][:2] != \"NN\"): #if noun doesn't already have an \"a\" or \"the\" in front of it and if it doesn't have another noun in front of it or a posessive pronouns like \"su\", waiting to be turned into noun+of+noun\n",
        "          taggedWords[idx:idx] = nltk.pos_tag(nltk.word_tokenize(\"the\"))\n",
        "          idx += 1 #so we don't recount nouns\n",
        "    idx += 1\n",
        "  #print(\"Add 'the':\", taggedWords)\n",
        "\n",
        "\n",
        "  #Rule 6: Attributive Nouns\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1:\n",
        "    if taggedWords[idx][1][:2] == \"NN\": #if current word is a noun\n",
        "      if taggedWords[idx+1][1][:2] == \"NN\": #if directly adjacent word is a noun\n",
        "        taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx] #swap the nouns\n",
        "        #print(taggedWords)\n",
        "        taggedWords[idx+1:idx+1] = nltk.pos_tag(nltk.word_tokenize(\"of\")) #insert \"of\" between the two nouns\n",
        "        #print(taggedWords)\n",
        "    idx += 1\n",
        "  #print(\"Attributive Nouns:\", taggedWords)\n",
        "\n",
        "  return(taggedWords)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ24BxdFuaLg"
      },
      "source": [
        "def getNounStart(endIndex, wordsArray):\n",
        "  nounStart = endIndex - 1\n",
        "  while True:\n",
        "    if endIndex-2 >= 0: #to avoid runtime errors\n",
        "      if (wordsArray[endIndex-2][1][:2] == \"DT\") | (wordsArray[endIndex-2][1][:2] == \"PRP$\") | (wordsArray[endIndex-2][1][:2] == \"POS\") | (wordsArray[endIndex-2][1][:2] == \"JJ\") | (wordsArray[endIndex-2][1][:2] == \"NN\"): #if there's a determiner or adjective next to the noun\n",
        "        nounStart = endIndex-2\n",
        "        if endIndex-3 >= 0: #to avoid runtime errors\n",
        "          if (wordsArray[endIndex-3][1][:2] == \"DT\") | (wordsArray[endIndex-3][1][:2] == \"PRP$\") | (wordsArray[endIndex-3][1][:2] == \"JJ\") | (wordsArray[endIndex-3][1][:2] == \"NN\"): #if there's a determiner next to the adjective\n",
        "            nounStart = endIndex-3\n",
        "            if endIndex-4 >=0:\n",
        "              if (wordsArray[endIndex-4][1][:2] == \"DT\") | (wordsArray[endIndex-4][1][:2] == \"PRP$\") | (wordsArray[endIndex-4][1][:2] == \"JJ\"):\n",
        "                nounStart = endIndex-4\n",
        "                if endIndex-5 >= 0:\n",
        "                  if (wordsArray[endIndex-5][1][:2] == \"DT\") | (wordsArray[endIndex-5][1][:2] == \"PRP$\"):\n",
        "                    nounStart = endIndex-5\n",
        "    \n",
        "    if nounStart-1 >= 0: #to avoid runtime errors\n",
        "      if wordsArray[nounStart-1][0] == \"of\":\n",
        "        endIndex = nounStart-1 #where noun's qualifier's start\n",
        "      else:\n",
        "        break\n",
        "    else:\n",
        "      break\n",
        "  return nounStart"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDJ0b90IagIm",
        "outputId": "00cd78b8-5f81-46aa-83ae-0f379bd70925"
      },
      "source": [
        "print(nltk.pos_tag(nltk.word_tokenize((\"The student's books\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('The', 'DT'), ('student', 'NN'), (\"'s\", 'POS'), ('books', 'NNS')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxFzmK0_a_wD"
      },
      "source": [
        "englishNegAndAff = [(\"no one\", \"someone\"), (\"nothing\", \"something\"), (\"none\", \"some\"), (\"neither\", \"also\"), (\"never\", \"always\"), (\"not\", \"\")] #no = exception. If no one --> someone. if no + noun --> none"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CStsjEEfJZW"
      },
      "source": [
        "spanishNegAndAff = [(\"nadie\", \"algien\"), (\"nada\", \"algo\"), (\"ningún\", \"algún\"), (\"ninguna\", \"alguna\"), (\"ningunas\", \"algunas\"), (\"ningunos\", \"algunos\"), (\"tampoco\", \"también\"), (\"nunca\", \"siempre\")] #exception = no"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRiyEVEOMikU"
      },
      "source": [
        "[nltk Acronym Meanings](https://www.guru99.com/pos-tagging-chunking-nltk.html#:~:text=POS%20Tagging%20(Parts%20of%20Speech,is%20also%20called%20grammatical%20tagging.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO9DQu_L_EvG"
      },
      "source": [
        "[Grammar Diff Btwn Spanish and English](https://www.thoughtco.com/grammatical-differences-between-spanish-and-english-4119326#:~:text=Word%20order%20is%20less%20fixed,subjunctive%20mood%20than%20English%20does.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA82S6sS5K_F"
      },
      "source": [
        "***Sometimes Pronouns Come Before Verbs and Nouns After***\n",
        "\n",
        "Lo escribió Cervantes.\n",
        "*   Megan wrote it (the book) --> It (the book) wrote Cervantes\n",
        "*   NNP VBD PRP --> PRP VBD NNP\n",
        "\n",
        "\"No recuerdo el momento en que salió Pablo\"\n",
        "*   I don't remember the moment in which Pablo left --> I don't remember the moment in which left Pablo\n",
        "*   (PRP VBP RB VB DT NN IN WDT) NNP VBD --> (same) VBD NNP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCp1Ijm678FV"
      },
      "source": [
        "***Spanish Uses Double Negatives***\n",
        "\n",
        "* Apenas come. (She barely eats.)\n",
        "* Apenas come nada. (She barely eats anything --> she barely eats nothing)\n",
        "* No tengo ninguno. (I don't have any --> I don't have none)\n",
        "* Nadie sabe eso. (Nobody knows that.)\n",
        "* Jamás fumo. (I never smoke.)\n",
        "* Tampoco comió. (She didn't eat either.)\n",
        "* Tampoco comió nada. (She didn't eat anything either --> She didn't eat nothing either)\n",
        "* No habló. (He didn't speak.)\n",
        "* No dijo nada. (He said nothing --> He didn't say nothing)\n",
        "* No le dijo nada a nadie. (He didn't say anything to anybody --> He didn't say nothing to nobody)\n",
        "* No compro ninguno. (I'm not buying any --> I'm not buying none)\n",
        "* Nunca le compra nada a nadie. (She never buys anything for anybody --> She never buy nothing for nobody)\n",
        "* No come ni siquiera pan. (He doesn't even eat bread --> He doesnt --honestly I don't even know how to translate this literally)\n",
        "* Ni siquiera come pan. (He doesn't even eat bread.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cNVR48c9tA0"
      },
      "source": [
        "***Can Omit Pronouns if Verb is Conjugated***\n",
        "\n",
        "* No compro ninguno. (I'm not buying any --> I'm not buying none)\n",
        "* Nunca le compra nada a nadie. (She never buys anything for anybody --> She never buy nothing for nobody)\n",
        "* No come ni siquiera pan. (He doesn't even eat bread --> He doesnt --honestly I don't even know how to translate this literally)\n",
        "* Ni siquiera come pan. (He doesn't even eat bread.)\n",
        "\n",
        "*Especially* Don't need \"I\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AASQ8qUr-lRI"
      },
      "source": [
        "***Attributive Nouns***\n",
        "Not coffee cup --> taza para cafe --> cup of coffee\n",
        "\n",
        "if two nouns next to each other --> switch nouns and add \"of\" (para/de) between\n",
        "\n",
        "* In some cases, this is accomplished by Spanish having adjectival forms that don't exist in English. For example, informático can be the equivalent of \"computer\" as an adjective, so a computer table is a mesa informática."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYO7dHT41p_s"
      },
      "source": [
        "***Tacking Pronouns to End of Verbs When \"To\"***\n",
        " * darmelo = give it to me --> give me it\n",
        " * escribamostelo = let's write it to you --> let's write you it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOOtV-htCnID",
        "outputId": "696ebedf-ce0f-4252-e527-acb74cc99b5e"
      },
      "source": [
        "tokenized = nltk.word_tokenize(\"el perro blanco corrió\")\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['el', 'perro', 'blanco', 'corrió']\n",
            "[('el', 'da0ms0'), ('perro', 'ncms000'), ('blanco', 'aq0ms0'), ('corrió', 'vmis3s0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFjLsmWmDCBe",
        "outputId": "6be2da8c-7bd7-450c-8860-62bf2167c70f"
      },
      "source": [
        "tokenized = nltk.word_tokenize(\"el perro azul corre\")\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['el', 'perro', 'azul', 'corre']\n",
            "[('el', 'da0ms0'), ('perro', 'ncms000'), ('azul', 'aq0cs0'), ('corre', 'vmip3s0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbpEhCZvC4qf",
        "outputId": "6a48439d-41a9-46d6-806d-46403e67d439"
      },
      "source": [
        "tokenized = nltk.word_tokenize(\"los perros feo corren\")\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['los', 'perros', 'feo', 'corren']\n",
            "[('los', 'da0mp0'), ('perros', 'ncmp000'), ('feo', 'aq0ms0'), ('corren', 'vmip3p0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wxrs8CkYEWbb",
        "outputId": "50f69515-b8b7-46c4-970d-985bdb27c84f"
      },
      "source": [
        "tokenized = nltk.word_tokenize(\"una chica bonitas corren\")\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['una', 'chica', 'bonitas', 'corren']\n",
            "[('una', 'di0fs0'), ('chica', 'ncfs000'), ('bonitas', None), ('corren', 'vmip3p0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi5V8qMpDPPk",
        "outputId": "76cdfe2d-1320-4910-915a-a9f654460ff0"
      },
      "source": [
        "tokenized = nltk.word_tokenize(\"Argentina corre\")\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Argentina', 'corre']\n",
            "[('Argentina', 'np0000l'), ('corre', 'vmip3s0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K_RFVdjEsS6"
      },
      "source": [
        "**Parsing POS Code Meaning**\n",
        "\n",
        "<u>Nouns</u>\n",
        "\n",
        "[0:2]\n",
        "* nc = regular noun\n",
        "* np = proper noun\n",
        "\n",
        "[3]\n",
        "* f = feminine\n",
        "* m = masculine\n",
        "\n",
        "[4]\n",
        "* s = singular\n",
        "* p = plural\n",
        "\n",
        "<u>Adjectives</u>\n",
        "\n",
        "[0:2] = aq\n",
        "\n",
        "[3]\n",
        "* f = feminine\n",
        "* m = masculine\n",
        "* c = gender nuetral\n",
        "\n",
        "[4]\n",
        "* s = singular\n",
        "* theoretically, p = plural, but spaghetti tagger doesn't seem to have many plural adjectives logged\n",
        "\n",
        "\n",
        "\n",
        "<u>Determiners</u>\n",
        "\n",
        "[0] = d\n",
        "\n",
        "[1]\n",
        "* a = el, la, los, las\n",
        "* i = un, una, unos, unas\n",
        "\n",
        "[2]\n",
        "* 0 = standard\n",
        "* 3 = possessive\n",
        "\n",
        "[3]\n",
        "* f = feminine\n",
        "* m = masculine\n",
        "\n",
        "[4]\n",
        "* s = singular\n",
        "* f = feminine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axYtC-o7DZ7G"
      },
      "source": [
        "**Notes**\n",
        "* Spaghetti tagger doesn't know people names\n",
        "* Its grammar is also wrong surrounding adjectives... doesn't perceive blancos as an adjective (sees it as noun) but percieves blanco as adjective, even when paired with perros\n",
        "* nor does it keep track of verbs conjugated in first person?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM2qSIHEIOid",
        "outputId": "ac692816-5843-4ece-ce0c-af6fad6c71db"
      },
      "source": [
        "spanglishMachine(\"se lo corrió\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: [('se', 'p0300000'), ('lo', 'da0ns0'), ('corrió', 'vmis3s0')]\n",
            "Adjectives before Nouns: [('se', 'p0300000'), ('lo', 'da0ns0'), ('corrió', 'vmis3s0')]\n",
            "Attributive/Possesive Nouns: [('se', 'p0300000'), ('lo', 'da0ns0'), ('corrió', 'vmis3s0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXyqCICjHPR1",
        "outputId": "1f91d136-f1ea-471f-b0c8-11baf5fa6092"
      },
      "source": [
        "spanglishMachine(\"café de Argentina\")"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: [('café', 'ncms000'), ('de', 'sps00'), ('Argentina', 'np0000l')]\n",
            "Adjectives before Nouns: [('café', 'ncms000'), ('de', 'sps00'), ('Argentina', 'np0000l')]\n",
            "1\n",
            "2\n",
            "4\n",
            "Attributive/Possesive Nouns: [('Argentina', 'np0000l'), (\"'\", 'apostophe'), ('de', 'sps00'), ('café', 'ncms000')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTwUFZWH2Ki4",
        "outputId": "7cefa87b-17d0-4116-e59e-f4bcc1d43090"
      },
      "source": [
        "spanglishMachine(\"taza de café\")"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: [('taza', 'ncfs000'), ('de', 'sps00'), ('café', 'ncms000')]\n",
            "Adjectives before Nouns: [('taza', 'ncfs000'), ('de', 'sps00'), ('café', 'ncms000')]\n",
            "1\n",
            "2\n",
            "3\n",
            "Attributive/Possesive Nouns: [('café', 'ncms000'), ('taza', 'ncfs000')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ll1gteCt_KZ"
      },
      "source": [
        "def toEnglishStyle(taggedWords):\n",
        "  #Rule 4: Nouns After Adjectives\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 1: #for every tuple (word) in Array\n",
        "    if taggedWords[idx][1][0] == \"n\": #if current word is an noun\n",
        "      if taggedWords[idx+1][1][:2] == \"aq\": #if the proceding word is a adjective\n",
        "        taggedWords[idx], taggedWords[idx+1] = taggedWords[idx+1], taggedWords[idx]\n",
        "    idx += 1\n",
        "  print(\"Adjectives before Nouns:\", taggedWords)\n",
        "  #print(\"under development\")\n",
        "\n",
        "  #omit la from beginning of plural direct objects\n",
        "\n",
        "  #attributive nouns (2nd noun = improper) and posessive nouns (2nd noun = proper)\n",
        "  idx = 0\n",
        "  while idx < len(taggedWords) - 2: #for every tuple (word) in Array\n",
        "    if taggedWords[idx][1][0] == \"n\": #if current word is an improper noun\n",
        "      if taggedWords[idx+1][0] == \"de\": #if the proceding word is \"de\"\n",
        "        if taggedWords[idx+2][1][:2] == \"nc\": #if the next word is an improper noun -- ATTRIBUTIVE NOUN\n",
        "          taggedWords[idx], taggedWords[idx+2] = taggedWords[idx+2], taggedWords[idx] #swap 1st and 2nd nouns\n",
        "          del taggedWords[idx+1] #delete \"de\"\n",
        "          #DEAL WITH FEM V MASC DETERMINERS\n",
        "          #if idx-1 >= 0:\n",
        "            #if taggedWords[idx-1][1][0][0] == \"f\":\n",
        "              #taggedWords[idx-1][1][0][0] == \"f\":\n",
        "        elif taggedWords[idx+2][1][:2] == \"np\": #if 2nd noun = proper --> POSSESSIVE NOUN\n",
        "          taggedWords[idx], taggedWords[idx+2] = taggedWords[idx+2], taggedWords[idx] #swap nouns\n",
        "          taggedWords[idx+1:idx+1] = [(\"'\", \"apostophe\")]\n",
        "    idx += 1\n",
        "  print(\"Attributive/Possesive Nouns:\", taggedWords)\n",
        "\n",
        "  #move around pronouns in front of verbs\n",
        "  "
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAPfUEMg6C4g"
      },
      "source": [
        "def getSpanNounEnd(dIndex, wordsArray):\n",
        "  nounEnd = dIndex+1\n",
        "  if endIndex+1 < len(wordsArray): #to avoid runtime errors\n",
        "    if (wordsArray[nounIndex+1][1][0] == \"n\") | (wordsArray[nounIndex+1][:2][0] == \"aq\"):\n",
        "      nounStart = nounEnd+1\n",
        "      if endIndex-2 >= 0: #to avoid runtime errors\n",
        "        if (wordsArray[nounIndex+2][1][0] == \"n\"):\n",
        "          nounStart = nounEnd+1\n",
        "  return nounEnd"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4yH_HBz0BZf"
      },
      "source": [
        "### **English Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJG4occ7x8Tb",
        "outputId": "73e3bec1-fccd-4fa2-c8ba-a5fd3ca4fa68"
      },
      "source": [
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7iAKLsWdED2"
      },
      "source": [
        "**Syntax** **trees**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFvVZ0c9k2mE"
      },
      "source": [
        "import os\n",
        "import matplotlib as mpl\n",
        "if os.environ.get('DISPLAY','') == '':\n",
        "    print('no display found. Using non-interactive Agg backend')\n",
        "    mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzqAOhi6lQug",
        "outputId": "05c289e1-bac0-4d6e-ee76-b80c9f91fdc1"
      },
      "source": [
        "### CREATE VIRTUAL DISPLAY ###\n",
        "!apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
            "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
            "  404  Not Found [IP: 91.189.88.142 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_1.19.6-1ubuntu4.8_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_PeZhw_i_Mj",
        "outputId": "ef68648f-5bcd-4557-fc6b-81af0754775d"
      },
      "source": [
        "# EXTRA STUFF TO DISPLAY NLTK SYNTAX TREES #  \n",
        "%matplotlib inline\n",
        "### INSTALL GHOSTSCRIPT ) ###\n",
        "!apt install ghostscript python3-tk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ghostscript is already the newest version (9.26~dfsg+0-0ubuntu0.18.04.14).\n",
            "python3-tk is already the newest version (3.6.9-1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "lP1mbYPsfyfG",
        "outputId": "3d6d68ee-cdbd-41b2-d3ff-8bf4e5a74989"
      },
      "source": [
        "chunked_sentence = '(S (NP this tree) (VP (V is) (AdjP pretty)))'\n",
        "\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display\n",
        "tree = Tree.fromstring(str(chunked_sentence))\n",
        "display(tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: couldn't connect to display \":1.0\""
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', ['this', 'tree']), Tree('VP', [Tree('V', ['is']), Tree('AdjP', ['pretty'])])])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g-Y9HWMycIR",
        "outputId": "f9aa8886-5011-4ef6-b1ad-6520e070ae67"
      },
      "source": [
        "Sentence = \"The white dog ran\"\n",
        "tokenized = nltk.word_tokenize(Sentence)\n",
        "print(tokenized)\n",
        "tagged = nltk.pos_tag(tokenized)\n",
        "print(tagged)\n",
        "chunkGram = r\"\"\"Chunk: {<DT\\w?>*<JJ\\w?>}\"\"\" #creating a chunk\n",
        "chunkParser = nltk.RegexpParser(chunkGram) #parsing the chunk\n",
        "#chunked = chunkParser.parse(tagged)\n",
        "for tree in chunkParser.parse(tagged): #creates a syntax tree for the parsed chunk\n",
        "    print(tree)\n",
        "    #tree.draw()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'white', 'dog', 'ran']\n",
            "[('The', 'DT'), ('white', 'JJ'), ('dog', 'NN'), ('ran', 'VBD')]\n",
            "(Chunk The/DT white/JJ)\n",
            "('dog', 'NN')\n",
            "('ran', 'VBD')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-95_ks94yZr",
        "outputId": "27a39db2-3585-4507-9c30-7db407cfbfee"
      },
      "source": [
        "Sentence = \"white the ran dog.\"\n",
        "tokenized = nltk.word_tokenize(Sentence)\n",
        "print(tokenized)\n",
        "tagged = nltk.pos_tag(tokenized)\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['white', 'the', 'ran', 'dog', '.']\n",
            "[('white', 'JJ'), ('the', 'DT'), ('ran', 'NN'), ('dog', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-kpFQz7J6uA"
      },
      "source": [
        "### **Spanish Data?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxr9qSXUSrAn",
        "outputId": "21fe10ac-58c4-46b8-ed35-3f71461c2796"
      },
      "source": [
        "!git clone https://github.com/alvations/spaghetti-tagger.git"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'spaghetti-tagger'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 33\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "-6UhXZTIXcCE",
        "outputId": "77ac09d6-8228-4d7f-9e26-3c0792f4d820"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/cont\n",
        "ent/drive')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-44-2e2577268db2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    drive.mount('/cont\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foieQg6LOart",
        "outputId": "8a9581e7-a880-4a32-875b-7ac483c7b6b7"
      },
      "source": [
        "nltk.corpus.cess_esp.words()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['El', 'grupo', 'estatal', 'Electricité_de_France', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tolSY-XOki1",
        "outputId": "9b2d5f97-641f-4e84-e147-8e6fe47ab0bb"
      },
      "source": [
        "nltk.corpus.cess_esp.sents()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdRSXGv6MLhA",
        "outputId": "0f260078-fccc-4992-aa36-0d4cd8565385"
      },
      "source": [
        "nltk.download(\"cess_esp\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Package cess_esp is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRUUYWeU1MlA"
      },
      "source": [
        "from nltk.corpus import cess_esp as cess\n",
        "from nltk import UnigramTagger as ut\n",
        "from nltk import BigramTagger as bt"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaDbFAtkOmKZ"
      },
      "source": [
        "# Read the corpus into a list, \n",
        "# each entry in the list is one sentence.\n",
        "cess_sents = cess.tagged_sents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBtEmt7FPZ8i"
      },
      "source": [
        "# Train the unigram tagger\n",
        "uni_tag = ut(cess_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEGYFga6P4No"
      },
      "source": [
        "spanishSentence = \"los perros azules corrion\""
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_tyHxvV1zBY",
        "outputId": "72fe864e-f9cf-47b5-b47b-d9d34652e565"
      },
      "source": [
        "#tokenized = spanishSentence.split()\n",
        "tokenized = nltk.word_tokenize(spanishSentence)\n",
        "print(tokenized)\n",
        "tagged1 = sp.pos_tag(tokenized)\n",
        "print(tagged1)\n",
        "tagged2 = mytagger_uni.tag(tokenized)\n",
        "print(tagged2)\n",
        "tagged4 = mytagger_bi.tag(tokenized)\n",
        "print(tagged4)\n",
        "print(tagged4[0])\n",
        "print(tagged4[0][0]) #How to parce through intervals of the tuple in the array"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['los', 'perros', 'azules', 'corrion']\n",
            "[('los', 'da0mp0'), ('perros', 'ncmp000'), ('azules', 'aq0cp0'), ('corrion', None)]\n",
            "[('los', 'da0mp0'), ('perros', 'ncmp000'), ('azules', 'aq0cp0'), ('corrion', None)]\n",
            "[('los', 'da0mp0'), ('perros', 'ncmp000'), ('azules', 'aq0cp0'), ('corrion', None)]\n",
            "('los', 'da0mp0')\n",
            "los\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEfuWamYEmjp",
        "outputId": "dca0d943-34b2-4067-8b7f-83771dfce414"
      },
      "source": [
        "femSentence = \"la niña es fea\"\n",
        "tokenized1 = femSentence.split()\n",
        "print(tokenized1)\n",
        "tagged3 = sp.pos_tag(tokenized1)\n",
        "print(tagged3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['la', 'niña', 'es', 'fea']\n",
            "[('la', 'da0fs0'), ('niña', 'ncfs000'), ('es', 'vsip3s0'), ('fea', 'aq0fs0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUsKWxc9Pvs7",
        "outputId": "3597b30d-3846-44ee-ab13-3a9b2a95d7e0"
      },
      "source": [
        "# Split corpus into training and testing set.\n",
        "train = int(len(cess_sents)*90/100) # 90%\n",
        "\n",
        "# Train a bigram tagger with only training data.\n",
        "bi_tag = bt(cess_sents[:train], backoff=uni_tag)\n",
        "\n",
        "# Evaluates on testing data remaining 10%\n",
        "bi_tag.evaluate(cess_sents[train+1:])\n",
        "\n",
        "# Using the tagger.\n",
        "bi_tag.tag(Sentence.split(\" \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('el', 'da0ms0'),\n",
              " ('perro', 'ncms000'),\n",
              " ('blanco', 'aq0ms0'),\n",
              " ('corrió', 'vmis3s0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "G4lJo9d67yBw",
        "outputId": "e502f828-e55b-438a-e084-375dd60b8e0b"
      },
      "source": [
        "nltk.corpus.genesis.tagged_words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-673971ba013a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PlaintextCorpusReader' object has no attribute 'tagged_words'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0AR8EKfCHbP"
      },
      "source": [
        "# **Sanskrit Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1IRb82_D-Cd"
      },
      "source": [
        "Mounting Drive and importing text files of Sanskrit texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvEDZkqnEK-R",
        "outputId": "cebce4f4-ec09-4855-8b7d-315cfff243fc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqsUhQPYCObp"
      },
      "source": [
        "gItA = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/bhagavadgItA.txt\",\"r\")\n",
        "meghadhUta = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/meghadUta.txt\",\"r\")\n",
        "rAmAyaNa = open(\"/content/drive/MyDrive/Colab Notebooks/ Final Project/Colab Notebooks/rAmAyaNa.txt\", \"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "snXpa2v0F_9D",
        "outputId": "2b15ed1b-3c47-4e86-f73d-67b8d43ec2f4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-b5768a72344c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pos/hmm/make -f makefile-osx\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv8b6SgbHmJa"
      },
      "source": [
        "comment to delete later: trying to figure out how to use repository here : https://github.com/ad2476/pos-research which has a sanskrit tagger. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcPj9dYhsxH3"
      },
      "source": [
        "## PYCHARM CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThEBL-XEs0Wv"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy\n",
        "import IPython\n",
        "from IPython import display\n",
        "\n",
        "# making sure ghostscript works correctly so we can correctly convert .ps to .png for syntax tree\n",
        "os.environ[\"PATH\"] += \"Library/usr/local/bin\"\n",
        "os.environ[\"PATH\"] = \"/usr/local/bin:\" + os.environ[\"PATH\"]\n",
        "os.environ[\"PATH\"] = \"Library/usr/local/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "\n",
        "#import necessary toolboxes\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "import pygame\n",
        "from pygame.locals import *\n",
        "\n",
        "#initialize user input of english sentence\n",
        "userSentence = input(\"Please enter a sentence under 15 words:\\n\")\n",
        "\n",
        "# create syntax tree that displays chunks\n",
        "def syntaxTreeGenerator(array):\n",
        "    for item in array:\n",
        "        tokenized = nltk.word_tokenize(item)\n",
        "        print(\"tokenized = \", tokenized)\n",
        "        tagged = nltk.pos_tag(tokenized)\n",
        "        print(\"tagged = \", tagged)\n",
        "\n",
        "        chunkGram = r\"\"\"Chunk: {<DT\\w?>*<JJ\\w?>}\"\"\" #creating a chunk\n",
        "        chunkParser = nltk.RegexpParser(chunkGram) #parsing the chunk\n",
        "\n",
        "        chunked = chunkParser.parse(tagged)\n",
        "        print(chunked)\n",
        "        #chunked.draw()\n",
        "\n",
        "        ### Save syntax tree as file ###\n",
        "        from nltk.draw.tree import TreeView\n",
        "        TreeView(chunked)._cframe.print_to_file('/Users/catherine/PycharmProjects/CLPS0950FinalProject/tree.ps')\n",
        "\n",
        "        ### Convert .ps file to .png\n",
        "        from PIL import Image\n",
        "        psimage = Image.open(r'/Users/catherine/PycharmProjects/CLPS0950FinalProject/tree.ps')\n",
        "        psimage.save(r'/Users/catherine/PycharmProjects/CLPS0950FinalProject/pygame_tree.png')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "syntaxTreeGenerator([userSentence])\n",
        "\n",
        "#initialize display window for pygame\n",
        "pygame.init()\n",
        "width, height = 1400, 720\n",
        "screen = pygame.display.set_mode((width, height))\n",
        "\n",
        "#define colors\n",
        "black = (0,0,0)\n",
        "white = (255, 255, 255)\n",
        "\n",
        "# pygame window name\n",
        "pygame.display.set_caption('Spanglish Generator')\n",
        "\n",
        "#font\n",
        "font = pygame.font.Font('freesansbold.ttf', 32)\n",
        "font2 = pygame.font.Font('freesansbold.ttf', 20)\n",
        "\n",
        "# text surface object\n",
        "titleText = font.render('Spanglish Generator', True, black, white)\n",
        "textRect = titleText.get_rect()\n",
        "\n",
        "syntaxTreeText = font2.render('Syntax Tree', True, black, white)\n",
        "rectSyntaxTreeText = syntaxTreeText.get_rect()\n",
        "#rectSyntaxTreeText.w = width//2\n",
        "\n",
        "\n",
        "# load images and center if need be\n",
        "background = pygame.image.load(\"/Users/catherine/Desktop/project_background.png\")\n",
        "syntaxTree = pygame.image.load(\"/Users/catherine/PycharmProjects/CLPS0950FinalProject/pygame_tree.png\")\n",
        "rectSyntaxTree = syntaxTree.get_rect()\n",
        "rectSyntaxTree.center = (width//2, 400)\n",
        "screen_rect = screen.get_rect()\n",
        "\n",
        "#rectSyntaxTree.center = (width//2, height//2)\n",
        "\n",
        "# loop through\n",
        "while True:\n",
        "    #allows user to exit screen\n",
        "    for event in pygame.event.get():\n",
        "        # check if event is the X button\n",
        "        if event.type == pygame.QUIT:\n",
        "            # if yes then quite game\n",
        "            pygame.quit()\n",
        "            sys.exit()\n",
        "    #clear the screen before putting things in again\n",
        "    screen.fill(white)\n",
        "\n",
        "    #draw screen elements\n",
        "    for x in range(width // background.get_width() + int(1)):\n",
        "        for y in range(height // background.get_height() + int(1)):\n",
        "            screen.blit(background, (x * 100, y * 100))\n",
        "    screen.blit(titleText, titleText.get_rect(midtop=screen_rect.midtop)) #says syntax generator at top of screen\n",
        "    screen.blit(syntaxTreeText, (640, 260))\n",
        "    #screen.blit(syntaxTreeText, syntaxTreeText.get_rect(w=rectSyntaxTreeText.w))\n",
        "    screen.blit(syntaxTree, syntaxTree.get_rect(center=rectSyntaxTree.center))\n",
        "\n",
        "\n",
        "    #screen.blit(print(syntaxTreeGenerator([userSentence])), (50, 50)) this doesn't work; shuts down\n",
        "\n",
        "\n",
        "    #Update the screen\n",
        "    pygame.display.flip()\n",
        "    # loop through events\n",
        "   # for event in pygame.event.get():\n",
        "        #check if event is the X button\n",
        "      #  if event.type==pygame.QUIT:\n",
        "            # if yes then quite game\n",
        "        #    pygame.quit()\n",
        "          #  sys.exit()\n",
        "\n",
        "\n",
        "            #quit()\n",
        "        #pygame.display.update()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#initialize user input of english sentence\n",
        "userSentence = input(\"Please enter a sentence:\\n\")\n",
        "#userSentenceList = list(userSentence.split(\" \"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create syntax tree that displays chunks\n",
        "def syntaxTreeGenerator(array):\n",
        "    for item in array:\n",
        "        tokenized = nltk.word_tokenize(item)\n",
        "        print(\"tokenized = \", tokenized)\n",
        "        tagged = nltk.pos_tag(tokenized)\n",
        "        print(\"tagged = \", tagged)\n",
        "\n",
        "        chunkGram = r\"\"\"Chunk: {<DT\\w?>*<JJ\\w?>}\"\"\" #creating a chunk\n",
        "        chunkParser = nltk.RegexpParser(chunkGram) #parsing the chunk\n",
        "\n",
        "        chunked = chunkParser.parse(tagged)\n",
        "        print(chunked)\n",
        "        chunked.draw()\n",
        "\n",
        "#calling the function\n",
        "exampleArray = [\"The white dog ran\"]\n",
        "syntaxTreeGenerator([userSentence])\n",
        "#syntaxTreeGenerator(exampleArray)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#for item in exampleArray:\n",
        "  #  tokenized = nltk.word_tokenize(item)\n",
        "  #  print(tokenized)\n",
        "   # tagged = nltk.pos_tag(tokenized)\n",
        "   # print(tagged)\n",
        "\n",
        "   # chunkGram = r\"\"\"Chunk: {<DT\\w?>*<JJ\\w?>}\"\"\" #creating a chunk\n",
        "   # chunkParser = nltk.RegexpParser(chunkGram) #parsing the chunk\n",
        "\n",
        "   # chunked = chunkParser.parse(tagged)\n",
        "   ## chunked.draw()\n",
        "\n",
        "#for tree in chunkParser.parse(tagged): #creates a syntax tree for the parsed chunk\n",
        "   # print(tree)\n",
        "   # chunked.draw()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}